{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzy/anaconda3/envs/python38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter, OrderedDict\n",
    "import random\n",
    "import matplotlib\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import obspy\n",
    "from scipy import signal\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def station_shuffled(ev_id,key,i=3,percentage=0.8):#筛选后的dataframe i 台站数大于等于多少  \n",
    "    # 将字符串数字转换为整数\n",
    "    ev_id_int = list(map(int, ev_id))\n",
    "\n",
    "    # 创建一个DataFrame\n",
    "    df = pd.DataFrame({'ev_id': ev_id_int, 'key': key})\n",
    "   \n",
    "    # 计算'key'列中每个数字的出现次数\n",
    "    counts = df['ev_id'].value_counts()\n",
    "    # 找到出现次数大于等于i次的数字\n",
    "    valid_values = counts[counts >= i].index\n",
    "\n",
    "    # 剔除重复的数字并按从小到大排列\n",
    "    unique_valid_values = sorted(set(valid_values))\n",
    "\n",
    "    # 打印剔除重复的数字后的列表\n",
    "    print(f'符合条件的总事件数{len(unique_valid_values)}')\n",
    "    \n",
    "    # 随机打乱列表\n",
    "    random.shuffle(unique_valid_values)\n",
    "   \n",
    "    total_elements = len(unique_valid_values)\n",
    "    train_evid=unique_valid_values[:int(total_elements * percentage)]\n",
    "\n",
    "    test_evid=unique_valid_values[int(total_elements * percentage):]\n",
    "\n",
    "    # 使用布尔索引删除出现次数少于i次的数字所在的行\n",
    "    filtered_df = df[df['ev_id'].isin(unique_valid_values)]\n",
    "    key_column=filtered_df['key']\n",
    "    train_list=[]\n",
    "    test_list=[]\n",
    "    for x in key_column:\n",
    "        key_correct = str(x).split('_')\n",
    "        k = key_correct[0]\n",
    "        if int(k) in train_evid:\n",
    "            train_list.append(x)\n",
    "        if int(k) in test_evid:\n",
    "            test_list.append(x)\n",
    "\n",
    "    return train_list,test_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据+筛选波形  返回合格波形与keylist\n",
    "def load_hdf5 (datalist,hdf5_path,json_file):\n",
    "    dataset = []  # 初始化一个空的列表来存储数据\n",
    "    # load the hdf5 file\n",
    "    data_list=[]\n",
    "    i=0\n",
    "    hdf5_file = h5py.File(hdf5_path, 'r')\n",
    "    for key in datalist :\n",
    "        \n",
    "        waveform = hdf5_file.get(key)[()]\n",
    "        information = json_file[key]\n",
    "\n",
    "        P = int(information['Pg'])\n",
    "        S = int(information['Sg'])\n",
    "        # 切片操作\n",
    "        sliced_waveform = waveform[P-500:S+2000,:]\n",
    "        if sliced_waveform.shape[0] <3000:\n",
    "            # 获取需要填充的零的数量\n",
    "            padding_size = 3000 - sliced_waveform.shape[0]\n",
    "            # 计算每列的均值\n",
    "            column_means = np.mean(sliced_waveform, axis=0)\n",
    "            # 创建一个填充后的数组\n",
    "            padded_waveform = np.zeros((3000, 3))\n",
    "            # 在每列下面填充相应列的均值\n",
    "            for k in range(3):\n",
    "                padded_waveform[:sliced_waveform.shape[0], k] = sliced_waveform[:, k]\n",
    "                padded_waveform[sliced_waveform.shape[0]:, k] = column_means[k]\n",
    "\n",
    "        else:\n",
    "            padded_waveform=sliced_waveform[:3000,:]\n",
    "\n",
    "        data = np.array(padded_waveform)\n",
    "        data=np.transpose(data)\n",
    "        \n",
    "        # data=preprocessing(data)\n",
    "\n",
    "        dataset.append(data)\n",
    "        data_list.append(key)\n",
    "    dataset=torch.tensor(dataset)\n",
    "    return dataset,data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#计算keylist中的事件数\n",
    "def list_count(id_list):\n",
    "    lisst=[]\n",
    "    for x in id_list:\n",
    "        key_correct = str(x).split('_')\n",
    "        k = key_correct[0]\n",
    "        lisst.append(k)\n",
    "\n",
    "    # 使用Counter计算每个字符串出现的次数\n",
    "    string_counts = Counter(lisst)\n",
    "    print(string_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取非天然地震数据\n",
    "def non_natural_data_read(hdf5_path,json_path):\n",
    "    # load the json file\n",
    "    non_natural_json_file = json.load(open(json_path, 'r'))\n",
    "    non_natural_list = list(non_natural_json_file.keys())\n",
    "    ep_idlist=[]\n",
    "    ss_idlist=[]\n",
    "    ep_list=[]\n",
    "    ss_list=[]\n",
    "    for i in non_natural_list :\n",
    "        # get the metadata from the json dictionary\n",
    "        information = non_natural_json_file[i]\n",
    "        if 'Pg' in information and 'Sg' in information:\n",
    "            key_correct = str(i).split('_')\n",
    "            key = key_correct[0]\n",
    "            if information['evtype']=='ep':\n",
    "                ep_list.append(key)\n",
    "                ep_idlist.append(i)       \n",
    "\n",
    "            elif information['evtype']=='ss':\n",
    "                ss_list.append(key)\n",
    "                ss_idlist.append(i)#key =event   i=event_id\n",
    "    ep_train_list,ep_test_list=station_shuffled(ep_list,ep_idlist)\n",
    "    ep_train_dataset,ep_train_id= load_hdf5(ep_train_list,hdf5_path,non_natural_json_file)\n",
    "    ep_test_dataset,ep_test_id= load_hdf5(ep_test_list,hdf5_path,non_natural_json_file)\n",
    "\n",
    "    print(f'ep_train_dataset.shape:{ep_train_dataset.shape}')\n",
    "    print(f'ep_test_dataset.shape:{ep_test_dataset.shape}')\n",
    "    ss_train_list,ss_test_list=station_shuffled(ss_list,ss_idlist)\n",
    "    ss_train_dataset,ss_train_id= load_hdf5(ss_train_list,hdf5_path,non_natural_json_file)\n",
    "    ss_test_dataset,ss_test_id= load_hdf5(ss_test_list,hdf5_path,non_natural_json_file)\n",
    "\n",
    "    print(f'ss_train_dataset.shape:{ss_train_dataset.shape}')\n",
    "    print(f'ss_test_dataset.shape:{ss_test_dataset.shape}')\n",
    "    return ep_train_dataset,ep_test_dataset,ss_train_dataset,ss_test_dataset,ep_train_id,ep_test_id,ss_train_id,ss_test_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取天然地震数据\n",
    "def natural_data_read(hdf5_path,json_path,num):\n",
    "    # load the json file\n",
    "    \n",
    "    natural_json_file = json.load(open(json_path, 'r'))\n",
    "    natural_key_list = list(natural_json_file.keys())\n",
    "    #print(len(natural_key_list))\n",
    "    # 随机取num个波形\n",
    "    natural_key_list = random.sample(natural_key_list, num)\n",
    "    natural_list=[]\n",
    "    natural_keylist=[]\n",
    "    for i in natural_key_list :\n",
    "        information = natural_json_file[i]\n",
    "        #确保ps到时同时存在  此处波形筛选information\n",
    "        if 'Pg' in information and 'Sg' in information:\n",
    "            key_correct = str(i).split('_')\n",
    "            key = key_correct[0]   \n",
    "            natural_list.append(key)\n",
    "            natural_keylist.append(i)\n",
    "    natural_train_list,natural_test_list=station_shuffled(natural_list,natural_keylist)\n",
    "    natural_train_dataset,natural_train_id= load_hdf5(natural_train_list,hdf5_path,natural_json_file)\n",
    "    natural_test_dataset,natural_test_id= load_hdf5(natural_test_list,hdf5_path,natural_json_file)\n",
    "    print(f'natural_train_dataset.shape:{natural_train_dataset.shape}')\n",
    "    print(f'natural_test_dataset.shape:{natural_test_dataset.shape}')\n",
    "\n",
    "    return natural_train_dataset,natural_test_dataset,natural_train_id,natural_test_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "符合条件的总事件数2426\n",
      "natural_train_dataset.shape:torch.Size([7029, 3, 3000])\n",
      "natural_test_dataset.shape:torch.Size([1794, 3, 3000])\n",
      "符合条件的总事件数501\n",
      "ep_train_dataset.shape:torch.Size([4411, 3, 3000])\n",
      "ep_test_dataset.shape:torch.Size([1112, 3, 3000])\n",
      "符合条件的总事件数357\n",
      "ss_train_dataset.shape:torch.Size([4271, 3, 3000])\n",
      "ss_test_dataset.shape:torch.Size([973, 3, 3000])\n"
     ]
    }
   ],
   "source": [
    "natural_hdf5_path='/home/zzy/Python projects/Dataset/DiTing2.0/CENC_DiTingv2_natural_earthquake.hdf5'\n",
    "natural_json_path='/home/zzy/Python projects/Dataset/DiTing2.0/CENC_DiTingv2_natural_earthquake.json'\n",
    "non_natural_hdf5_path='/home/zzy/Python projects/Dataset/DiTing2.0/CENC_DiTingv2_non_natural_earthquake.hdf5'\n",
    "non_natural_json_path='/home/zzy/Python projects/Dataset/DiTing2.0/CENC_DiTingv2_non_natural_earthquake.json'\n",
    "\n",
    "natural_train_dataset,natural_test_dataset,natural_train_id,natural_test_id=natural_data_read(natural_hdf5_path,natural_json_path,100000)\n",
    "\n",
    "ep_train_dataset,ep_test_dataset,ss_train_dataset,ss_test_dataset,ep_train_id,ep_test_id,ss_train_id,ss_test_id=non_natural_data_read(non_natural_hdf5_path,non_natural_json_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去趋势\n",
    "def detrend(data):\n",
    "    detrended_data = signal.detrend(data, axis=-1)\n",
    "    return detrended_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滤波  采样率50hz  1-20hz带通滤波\n",
    "def filter(data, fs=50, lowcut=1, highcut=20):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = signal.butter(4, [low, high], btype='band')#带通\n",
    "    #b, a = signal.butter(4, high, 'lowpass')#低通\n",
    "    filtered_data = signal.filtfilt(b, a, data, axis=-1)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 Z-Score 对输入数据进行标准化\n",
    "def z_score_normalize(data):\n",
    "    \n",
    "    mean_value = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    \n",
    "    # 检查标准差是否为零，避免除以零的情况\n",
    "    if std_dev == 0:\n",
    "        # 如果标准差为零，直接返回原始数据\n",
    "        return data\n",
    "    \n",
    "    normalized_data = (data - mean_value) / std_dev\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing (tensor_data,lowcut = 1,highcut = 20):\n",
    "\n",
    "    for i in range(tensor_data.size(0)):\n",
    "\n",
    "        for j in range(tensor_data.size(1)):\n",
    "            # 获取当前通道和样本的数据\n",
    "            data = tensor_data[i, j, :].numpy()\n",
    "            # 对数据进行去趋势处理\n",
    "            detrended_data = detrend(data)\n",
    "            # 对去趋势后的数据进行1-25Hz滤波处理\n",
    "            filtered_data = filter(detrended_data,lowcut = lowcut,highcut = highcut)\n",
    "             # 归一化处理\n",
    "            normalized_data = z_score_normalize(filtered_data)\n",
    "            # 将结果赋值回原始张量\n",
    "            tensor_data[i, j, :] = torch.tensor(np.ascontiguousarray(normalized_data))\n",
    "            \n",
    "    return tensor_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_train_dataset=preprocessing(natural_train_dataset)\n",
    "natural_test_dataset=preprocessing(natural_test_dataset)\n",
    "ep_train_dataset=preprocessing(ep_train_dataset)\n",
    "ep_test_dataset=preprocessing(ep_test_dataset)\n",
    "ss_train_dataset=preprocessing(ss_train_dataset)\n",
    "ss_test_dataset=preprocessing(ss_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data save plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存+画图\n",
    "base_path='/home/zzy/Python projects/Dataset/1'\n",
    "# 从数据中随机选择50个波形样本 plot\n",
    "import os\n",
    "for i in range(50):\n",
    "    # 从数据中随机选择一个样本\n",
    "    selected_sample = random.randint(0, natural_train_dataset.size(0) - 1)\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(16,8))\n",
    "\n",
    "    ax[0].plot(natural_train_dataset[selected_sample, 0,:])\n",
    "    ax[1].plot(natural_train_dataset[selected_sample, 1,:])\n",
    "    ax[2].plot(natural_train_dataset[selected_sample, 2,:])\n",
    "    ax[0].set_title('Z')\n",
    "    ax[1].set_title('N')\n",
    "    ax[2].set_title('E')\n",
    "\n",
    "    # 保存图像到文件夹\n",
    "    output_path = os.path.join(base_path+'/output/natural_train', f'natural_image_{i}.png')#更改路径\n",
    "    plt.savefig(output_path)\n",
    "    # close the figure\n",
    "    plt.close(fig)\n",
    "for i in range(50):\n",
    "    # 从数据中随机选择一个样本\n",
    "    selected_sample = random.randint(0, ep_train_dataset.size(0) - 1)\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(16,8))\n",
    "\n",
    "    ax[0].plot(ep_train_dataset[selected_sample, 0,:])\n",
    "    ax[1].plot(ep_train_dataset[selected_sample, 1,:])\n",
    "    ax[2].plot(ep_train_dataset[selected_sample, 2,:])\n",
    "    ax[0].set_title('Z')\n",
    "    ax[1].set_title('N')\n",
    "    ax[2].set_title('E')\n",
    "\n",
    "    # 保存图像到文件夹\n",
    "    output_path = os.path.join(base_path+'/output/ep_train', f'ep_image_{i}.png')#更改路径\n",
    "    plt.savefig(output_path)\n",
    "    # close the figure\n",
    "    plt.close(fig)\n",
    "for i in range(50):\n",
    "    # 从数据中随机选择一个样本\n",
    "    selected_sample = random.randint(0, ss_train_dataset.size(0) - 1)\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(16,8))\n",
    "\n",
    "    ax[0].plot(ss_train_dataset[selected_sample, 0,:])\n",
    "    ax[1].plot(ss_train_dataset[selected_sample, 1,:])\n",
    "    ax[2].plot(ss_train_dataset[selected_sample, 2,:])\n",
    "    ax[0].set_title('Z')\n",
    "    ax[1].set_title('N')\n",
    "    ax[2].set_title('E')\n",
    "\n",
    "    # 保存图像到文件夹\n",
    "    output_path = os.path.join(base_path+'/output/ss_train', f'ss_image_{i}.png')#更改路径\n",
    "    plt.savefig(output_path)\n",
    "    # close the figure\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 保存张量到文件\n",
    "torch.save(natural_train_dataset, base_path+'/tensor dataset/natural_train_dataset.pt')\n",
    "torch.save(natural_test_dataset, base_path+'/tensor dataset/natural_test_dataset.pt')\n",
    "torch.save(ep_train_dataset, base_path+'/tensor dataset/ep_train_dataset.pt')\n",
    "torch.save(ep_test_dataset, base_path+'/tensor dataset/ep_test_dataset.pt')\n",
    "torch.save(ss_train_dataset, base_path+'/tensor dataset/ss_train_dataset.pt')\n",
    "torch.save(ss_test_dataset, base_path+'/tensor dataset/ss_test_dataset.pt')\n",
    "#保存key \n",
    "torch.save(natural_train_id, base_path+'/tensor dataset/natural_train_id.pt')\n",
    "torch.save(natural_test_id, base_path+'/tensor dataset/natural_test_id.pt')\n",
    "torch.save(ep_train_id, base_path+'/tensor dataset/ep_train_id.pt')\n",
    "torch.save(ep_test_id, base_path+'/tensor dataset/ep_test_id.pt')\n",
    "torch.save(ss_train_id, base_path+'/tensor dataset/ss_train_id.pt')\n",
    "torch.save(ss_test_id, base_path+'/tensor dataset/ss_test_id.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
