{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#噪声数据集数据集结构\n",
    "import h5py\n",
    "\n",
    "# HDF5 文件路径\n",
    "file_path = '/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_noise_set.hdf5'\n",
    "\n",
    "# 打开 HDF5 文件\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # 打印文件的结构，列出所有组和数据集\n",
    "    print(\"文件结构：\")\n",
    "    def print_structure(name, obj):\n",
    "        print(f\"{name}: {obj}\")\n",
    "    \n",
    "    f.visititems(print_structure)  # 访问并打印文件的所有项（组和数据集）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# 读取 HDF5 文件路径\n",
    "file_path = '/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_noise_set.hdf5'\n",
    "\n",
    "# 打开文件\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # 初始化字典来统计每个专家的噪声数据集数量\n",
    "    expert_data_count = {\n",
    "        \"Junior_A\": 0,\n",
    "        \"Expert_A\": 0,\n",
    "        \"Expert_B\": 0,\n",
    "        \"Expert_C\": 0\n",
    "    }\n",
    "    \n",
    "    # 遍历所有数据集\n",
    "    for dataset_name in f.keys():\n",
    "        # 根据数据集名称判断属于哪个专家的噪声数据集\n",
    "        if \"Junior_A_Noise\" in dataset_name:\n",
    "            expert_data_count[\"Junior_A\"] += 1\n",
    "        elif \"Expert_A_Noise\" in dataset_name:\n",
    "            expert_data_count[\"Expert_A\"] += 1\n",
    "        elif \"Expert_B_Noise\" in dataset_name:\n",
    "            expert_data_count[\"Expert_B\"] += 1\n",
    "        elif \"Expert_C_Noise\" in dataset_name:\n",
    "            expert_data_count[\"Expert_C\"] += 1\n",
    "\n",
    "# 打印每个专家的噪声数据集数量\n",
    "print(\"每个专家对应的噪声数据集数量：\")\n",
    "for expert, count in expert_data_count.items():\n",
    "    print(f\"{expert}: {count} 个数据集\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# 读取 HDF5 文件路径\n",
    "file_path = '/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_noise_set.hdf5'\n",
    "\n",
    "# 打开文件\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # 初始化字典来统计每个专家的数据集采样点分布（更细化的区间）\n",
    "    expert_sample_distribution = {\n",
    "        \"Junior_A\": {\"0-500\": 0, \"500-1000\": 0, \"1000-1500\": 0, \"1500-2000\": 0, \"2000-2500\": 0, \"2500-3000\": 0, \"3000+\": 0},\n",
    "        \"Expert_A\": {\"0-500\": 0, \"500-1000\": 0, \"1000-1500\": 0, \"1500-2000\": 0, \"2000-2500\": 0, \"2500-3000\": 0, \"3000+\": 0},\n",
    "        \"Expert_B\": {\"0-500\": 0, \"500-1000\": 0, \"1000-1500\": 0, \"1500-2000\": 0, \"2000-2500\": 0, \"2500-3000\": 0, \"3000+\": 0},\n",
    "        \"Expert_C\": {\"0-500\": 0, \"500-1000\": 0, \"1000-1500\": 0, \"1500-2000\": 0, \"2000-2500\": 0, \"2500-3000\": 0, \"3000+\": 0}\n",
    "    }\n",
    "\n",
    "    # 遍历所有数据集\n",
    "    for dataset_name in f.keys():\n",
    "        dataset = f[dataset_name]\n",
    "        num_samples = dataset.shape[0]  # 获取采样点数量\n",
    "\n",
    "        # 根据数据集名称判断属于哪个专家的噪声数据集\n",
    "        if \"Junior_A_Noise\" in dataset_name:\n",
    "            if num_samples <= 500:\n",
    "                expert_sample_distribution[\"Junior_A\"][\"0-500\"] += 1\n",
    "            elif num_samples <= 1000:\n",
    "                expert_sample_distribution[\"Junior_A\"][\"500-1000\"] += 1\n",
    "            elif num_samples <= 1500:\n",
    "                expert_sample_distribution[\"Junior_A\"][\"1000-1500\"] += 1\n",
    "            elif num_samples <= 2000:\n",
    "                expert_sample_distribution[\"Junior_A\"][\"1500-2000\"] += 1\n",
    "            elif num_samples <= 2500:\n",
    "                expert_sample_distribution[\"Junior_A\"][\"2000-2500\"] += 1\n",
    "            elif num_samples <= 3000:\n",
    "                expert_sample_distribution[\"Junior_A\"][\"2500-3000\"] += 1\n",
    "            else:\n",
    "                expert_sample_distribution[\"Junior_A\"][\"3000+\"] += 1\n",
    "\n",
    "        elif \"Expert_A_Noise\" in dataset_name:\n",
    "            if num_samples <= 500:\n",
    "                expert_sample_distribution[\"Expert_A\"][\"0-500\"] += 1\n",
    "            elif num_samples <= 1000:\n",
    "                expert_sample_distribution[\"Expert_A\"][\"500-1000\"] += 1\n",
    "            elif num_samples <= 1500:\n",
    "                expert_sample_distribution[\"Expert_A\"][\"1000-1500\"] += 1\n",
    "            elif num_samples <= 2000:\n",
    "                expert_sample_distribution[\"Expert_A\"][\"1500-2000\"] += 1\n",
    "            elif num_samples <= 2500:\n",
    "                expert_sample_distribution[\"Expert_A\"][\"2000-2500\"] += 1\n",
    "            elif num_samples <= 3000:\n",
    "                expert_sample_distribution[\"Expert_A\"][\"2500-3000\"] += 1\n",
    "            else:\n",
    "                expert_sample_distribution[\"Expert_A\"][\"3000+\"] += 1\n",
    "\n",
    "        elif \"Expert_B_Noise\" in dataset_name:\n",
    "            if num_samples <= 500:\n",
    "                expert_sample_distribution[\"Expert_B\"][\"0-500\"] += 1\n",
    "            elif num_samples <= 1000:\n",
    "                expert_sample_distribution[\"Expert_B\"][\"500-1000\"] += 1\n",
    "            elif num_samples <= 1500:\n",
    "                expert_sample_distribution[\"Expert_B\"][\"1000-1500\"] += 1\n",
    "            elif num_samples <= 2000:\n",
    "                expert_sample_distribution[\"Expert_B\"][\"1500-2000\"] += 1\n",
    "            elif num_samples <= 2500:\n",
    "                expert_sample_distribution[\"Expert_B\"][\"2000-2500\"] += 1\n",
    "            elif num_samples <= 3000:\n",
    "                expert_sample_distribution[\"Expert_B\"][\"2500-3000\"] += 1\n",
    "            else:\n",
    "                expert_sample_distribution[\"Expert_B\"][\"3000+\"] += 1\n",
    "\n",
    "        elif \"Expert_C_Noise\" in dataset_name:\n",
    "            if num_samples <= 500:\n",
    "                expert_sample_distribution[\"Expert_C\"][\"0-500\"] += 1\n",
    "            elif num_samples <= 1000:\n",
    "                expert_sample_distribution[\"Expert_C\"][\"500-1000\"] += 1\n",
    "            elif num_samples <= 1500:\n",
    "                expert_sample_distribution[\"Expert_C\"][\"1000-1500\"] += 1\n",
    "            elif num_samples <= 2000:\n",
    "                expert_sample_distribution[\"Expert_C\"][\"1500-2000\"] += 1\n",
    "            elif num_samples <= 2500:\n",
    "                expert_sample_distribution[\"Expert_C\"][\"2000-2500\"] += 1\n",
    "            elif num_samples <= 3000:\n",
    "                expert_sample_distribution[\"Expert_C\"][\"2500-3000\"] += 1\n",
    "            else:\n",
    "                expert_sample_distribution[\"Expert_C\"][\"3000+\"] += 1\n",
    "\n",
    "# 打印每个专家的采样点分布\n",
    "print(\"每个专家的数据集采样点分布：\")\n",
    "for expert, distribution in expert_sample_distribution.items():\n",
    "    print(f\"\\n{expert}:\")\n",
    "    for range, count in distribution.items():\n",
    "        print(f\"  {range}: {count} 个数据集\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# HDF5 文件路径\n",
    "file_path = '/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_noise_set.hdf5'\n",
    "\n",
    "# 保存的文件夹路径\n",
    "save_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset'\n",
    "\n",
    "# 创建保存路径（如果不存在的话）\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 打开HDF5文件\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # 遍历所有数据集\n",
    "    for dataset_name in f.keys():\n",
    "        # 仅处理噪声数据集，过滤掉与地震（Earthquake）相关的数据集\n",
    "        if 'Noise' in dataset_name:\n",
    "            dataset = f[dataset_name]\n",
    "            \n",
    "            # 提取数据\n",
    "            data = np.array(dataset)\n",
    "            \n",
    "            # 保存为.npy文件，命名为数据集名称\n",
    "            save_path = os.path.join(save_dir, f\"{dataset_name}.npy\")\n",
    "            np.save(save_path, data)  # 保存数据为.npy格式\n",
    "            \n",
    "            print(f\"Saved: {save_path}\")\n",
    "\n",
    "print(\"所有噪声数据集已提取并保存为.npy文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 文件夹路径\n",
    "input_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step1'\n",
    "output_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step2'\n",
    "\n",
    "# 创建输出文件夹（如果不存在）\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 获取所有文件的路径\n",
    "all_files = os.listdir(input_dir)\n",
    "\n",
    "# 遍历所有.npy文件\n",
    "for file_name in all_files:\n",
    "    if file_name.endswith('.npy'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # 加载数据\n",
    "        data = np.load(file_path)\n",
    "\n",
    "        # 获取数据的采样点数量\n",
    "        num_samples = data.shape[0]\n",
    "\n",
    "        # 如果数据采样点小于3000，进行均值填充；如果大于3000，进行裁剪\n",
    "        if num_samples < 3000:\n",
    "            # 计算每个通道的均值\n",
    "            mean_values = np.mean(data, axis=0)\n",
    "            # 计算需要填充的数量\n",
    "            padding_length = 3000 - num_samples\n",
    "            # 用均值填充到尾部\n",
    "            padded_data = np.tile(mean_values, (padding_length, 1))\n",
    "            # 拼接数据\n",
    "            new_data = np.vstack((data, padded_data))\n",
    "        \n",
    "        elif num_samples > 3000:\n",
    "            # 截取前3000个采样点\n",
    "            new_data = data[:3000, :]\n",
    "        \n",
    "        else:\n",
    "            # 数据已经是3000个采样点，无需处理\n",
    "            new_data = data\n",
    "\n",
    "        # 生成新的文件名，添加_step2后缀\n",
    "        new_file_name = file_name.replace('.npy', '_step2.npy')\n",
    "        new_file_path = os.path.join(output_dir, new_file_name)\n",
    "\n",
    "        # 保存处理后的数据为.npy文件\n",
    "        np.save(new_file_path, new_data)\n",
    "\n",
    "        print(f\"Processed and saved: {new_file_path}\")\n",
    "\n",
    "print(\"所有数据处理完成并保存为新的.npy文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 文件夹路径\n",
    "output_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step2'\n",
    "\n",
    "# 获取所有文件的路径\n",
    "all_files = os.listdir(output_dir)\n",
    "\n",
    "# 遍历所有文件，删除通道数为1的文件\n",
    "deleted_files = []  # 用来记录删除的文件\n",
    "\n",
    "for file_name in all_files:\n",
    "    if file_name.endswith('.npy'):\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        data = np.load(file_path)\n",
    "        \n",
    "        # 检查数据的通道数是否为1\n",
    "        if data.shape[1] == 1:\n",
    "            os.remove(file_path)  # 删除文件\n",
    "            deleted_files.append(file_name)  # 记录删除的文件名\n",
    "\n",
    "# 输出删除的文件列表\n",
    "print(f\"删除了 {len(deleted_files)} 个文件：\")\n",
    "for deleted_file in deleted_files:\n",
    "    print(deleted_file)\n",
    "\n",
    "print(\"删除操作完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 文件夹路径\n",
    "output_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step2'\n",
    "\n",
    "# 获取所有文件的路径\n",
    "all_files = os.listdir(output_dir)\n",
    "\n",
    "# 初始化字典来存储每个专家的文件数量\n",
    "expert_data_count = {\n",
    "    \"Junior_A\": 0,\n",
    "    \"Expert_A\": 0,\n",
    "    \"Expert_B\": 0,\n",
    "    \"Expert_C\": 0\n",
    "}\n",
    "\n",
    "# 遍历所有文件，检查数据形状并分类\n",
    "print(\"文件数据形状（采样点，通道数）以及专家对应的文件数量：\")\n",
    "for file_name in all_files:\n",
    "    if file_name.endswith('.npy'):\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        # 加载数据\n",
    "        data = np.load(file_path)\n",
    "        num_samples, num_channels = data.shape  # 获取数据的采样点和通道数\n",
    "        \n",
    "        # 打印每个文件的数据形状\n",
    "        print(f\"文件名: {file_name}, 数据形状: ({num_samples}, {num_channels})\")\n",
    "        \n",
    "        # 根据文件名判断专家并统计数据数量\n",
    "        if 'Junior_A' in file_name:\n",
    "            expert_data_count[\"Junior_A\"] += 1\n",
    "        elif 'Expert_A' in file_name:\n",
    "            expert_data_count[\"Expert_A\"] += 1\n",
    "        elif 'Expert_B' in file_name:\n",
    "            expert_data_count[\"Expert_B\"] += 1\n",
    "        elif 'Expert_C' in file_name:\n",
    "            expert_data_count[\"Expert_C\"] += 1\n",
    "\n",
    "# 打印每个专家的数据数量\n",
    "print(\"\\n每个专家对应的数据个数：\")\n",
    "for expert, count in expert_data_count.items():\n",
    "    print(f\"{expert}: {count} 个数据集\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 文件夹路径\n",
    "input_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step2'\n",
    "output_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step3'\n",
    "\n",
    "# 创建输出文件夹（如果不存在）\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 获取所有文件的路径\n",
    "all_files = os.listdir(input_dir)\n",
    "\n",
    "# 遍历所有文件，进行重新存储\n",
    "for file_name in all_files:\n",
    "    if file_name.endswith('.npy'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # 加载数据\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        \n",
    "        # 检查数据格式是否是字典\n",
    "        if isinstance(data, dict):\n",
    "            # 获取E、N、Z通道数据并合并为二维数组 (3000, 3)\n",
    "            E_channel = data['E']\n",
    "            N_channel = data['N']\n",
    "            Z_channel = data['Z']\n",
    "            \n",
    "            # 合并为一个二维数组\n",
    "            new_data = np.column_stack((E_channel, N_channel, Z_channel))\n",
    "        \n",
    "        # 如果是NumPy数组格式，直接处理\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            # 直接将数据（假设数据已经是(3000, 3)）重构为二维数组\n",
    "            new_data = data\n",
    "        \n",
    "        else:\n",
    "            print(f\"文件 {file_name} 的数据格式不符合预期\")\n",
    "            continue\n",
    "        \n",
    "        # 生成新的文件名，添加_step3后缀\n",
    "        new_file_name = file_name.replace('_step2.npy', '_step3.npy')\n",
    "        new_file_path = os.path.join(output_dir, new_file_name)\n",
    "        \n",
    "        # 保存处理后的数据为.npy文件\n",
    "        np.save(new_file_path, new_data)\n",
    "\n",
    "        print(f\"Processed and saved: {new_file_path}\")\n",
    "\n",
    "print(\"所有数据处理完成并保存为新的.npy文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 文件夹路径\n",
    "output_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step3'\n",
    "\n",
    "# 获取所有文件的路径\n",
    "all_files = os.listdir(output_dir)\n",
    "\n",
    "# 随机选择3个文件进行打印\n",
    "random_files = random.sample([file for file in all_files if file.endswith('.npy')], 3)\n",
    "\n",
    "print(\"随机打印几个数据文件的内容：\")\n",
    "\n",
    "# 遍历随机选中的文件，打印每个文件的内容\n",
    "for file_name in random_files:\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    # 加载数据，允许加载包含对象的 .npy 文件\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # 打印文件名和数据\n",
    "    print(f\"\\n文件名: {file_name}\")\n",
    "    print(f\"数据形状: {data.shape}\")\n",
    "    \n",
    "    # 打印每个通道的完整时间序列数据\n",
    "    print(f\"E通道数据（所有3000个采样点）：\\n{data[:, 0]}\")\n",
    "    print(f\"N通道数据（所有3000个采样点）：\\n{data[:, 1]}\")\n",
    "    print(f\"Z通道数据（所有3000个采样点）：\\n{data[:, 2]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去趋势，带通滤波，Z标准化\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# 带通滤波函数\n",
    "def bandpass_filter(data, lowcut, highcut, fs=50, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "# 文件夹路径\n",
    "input_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step3'\n",
    "output_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step4'\n",
    "\n",
    "# 创建输出文件夹（如果不存在）\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 获取所有文件的路径\n",
    "all_files = os.listdir(input_dir)\n",
    "\n",
    "# 遍历所有文件，进行预处理（去趋势、带通滤波、Z标准化）\n",
    "for file_name in all_files:\n",
    "    if file_name.endswith('.npy'):\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        \n",
    "        # 加载数据\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        \n",
    "        # 获取E、N、Z通道数据\n",
    "        E_channel = data[:, 0]\n",
    "        N_channel = data[:, 1]\n",
    "        Z_channel = data[:, 2]\n",
    "        \n",
    "        # 1. 去趋势处理：减去均值\n",
    "        E_channel -= np.mean(E_channel)\n",
    "        N_channel -= np.mean(N_channel)\n",
    "        Z_channel -= np.mean(Z_channel)\n",
    "\n",
    "        # 2. 带通滤波：1-20Hz\n",
    "        E_channel = bandpass_filter(E_channel, lowcut=1, highcut=20, fs=50)\n",
    "        N_channel = bandpass_filter(N_channel, lowcut=1, highcut=20, fs=50)\n",
    "        Z_channel = bandpass_filter(Z_channel, lowcut=1, highcut=20, fs=50)\n",
    "\n",
    "        # 3. Z标准化：标准化到零均值和单位方差\n",
    "        # 检查标准差是否为0，避免除以0的错误\n",
    "        if np.std(E_channel) != 0:\n",
    "            E_channel = (E_channel - np.mean(E_channel)) / np.std(E_channel)\n",
    "        else:\n",
    "            E_channel = np.zeros_like(E_channel)  # 或者选择其他填充策略\n",
    "\n",
    "        if np.std(N_channel) != 0:\n",
    "            N_channel = (N_channel - np.mean(N_channel)) / np.std(N_channel)\n",
    "        else:\n",
    "            N_channel = np.zeros_like(N_channel)  # 或者选择其他填充策略\n",
    "\n",
    "        if np.std(Z_channel) != 0:\n",
    "            Z_channel = (Z_channel - np.mean(Z_channel)) / np.std(Z_channel)\n",
    "        else:\n",
    "            Z_channel = np.zeros_like(Z_channel)  # 或者选择其他填充策略\n",
    "        \n",
    "        # 合并E、N、Z通道数据为二维数组 (3000, 3)\n",
    "        new_data = np.column_stack((E_channel, N_channel, Z_channel))\n",
    "        \n",
    "        # 生成新的文件名，添加_step4后缀\n",
    "        new_file_name = file_name.replace('_step3.npy', '_step4.npy')\n",
    "        new_file_path = os.path.join(output_dir, new_file_name)\n",
    "        \n",
    "        # 保存处理后的数据为.npy文件\n",
    "        np.save(new_file_path, new_data)\n",
    "\n",
    "        print(f\"Processed and saved: {new_file_path}\")\n",
    "\n",
    "print(\"所有数据处理完成并保存为新的.npy文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 文件夹路径\n",
    "output_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step4'\n",
    "\n",
    "# 获取所有文件的路径\n",
    "all_files = os.listdir(output_dir)\n",
    "\n",
    "# 随机选择3个文件进行可视化\n",
    "random_files = random.sample([file for file in all_files if file.endswith('.npy')], 3)\n",
    "\n",
    "# 使用Matplotlib的默认样式\n",
    "plt.style.use('classic')\n",
    "\n",
    "# 遍历随机选中的文件，绘制波形\n",
    "for file_name in random_files:\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    \n",
    "    # 加载数据\n",
    "    data = np.load(file_path)\n",
    "    \n",
    "    # 获取每个通道的数据\n",
    "    E_channel = data[:, 0]\n",
    "    N_channel = data[:, 1]\n",
    "    Z_channel = data[:, 2]\n",
    "    \n",
    "    # 创建一个新的图形\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 绘制E通道波形\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(E_channel, label=\"E Channel\", color='blue')\n",
    "    plt.title(f'{file_name} - E Channel')\n",
    "    plt.xlabel('Sample Points')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    # 绘制N通道波形\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(N_channel, label=\"N Channel\", color='green')\n",
    "    plt.title(f'{file_name} - N Channel')\n",
    "    plt.xlabel('Sample Points')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    # 绘制Z通道波形\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(Z_channel, label=\"Z Channel\", color='red')\n",
    "    plt.title(f'{file_name} - Z Channel')\n",
    "    plt.xlabel('Sample Points')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    # 显示所有图形\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自然地震数据集具体内容\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 文件路径\n",
    "dataset_path = '/home/zypei/DiTing2.0_dataset/tensor_dataset/natural_train_dataset.pt'\n",
    "\n",
    "# 检查文件是否存在\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"文件存在: {dataset_path}\")\n",
    "    # 加载数据\n",
    "    dataset = torch.load(dataset_path)\n",
    "    \n",
    "    # 打印数据的结构和内容\n",
    "    print(\"数据内容：\")\n",
    "    print(dataset)\n",
    "else:\n",
    "    print(f\"文件未找到: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自然地震，爆炸塌陷样式\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 文件路径\n",
    "natural_dataset_path = '/home/zypei/DiTing2.0_dataset/tensor_dataset/natural_train_dataset.pt'  # 地震数据集\n",
    "ep_dataset_path = '/home/zypei/DiTing2.0_dataset/tensor_dataset/ep_train_dataset.pt'  # 爆炸数据集\n",
    "ss_dataset_path = '/home/zypei/DiTing2.0_dataset/tensor_dataset/ss_train_dataset.pt'  # 塌陷数据集\n",
    "\n",
    "# 加载并绘制数据\n",
    "def load_and_plot(dataset_path, title_prefix=\"Sample\"):\n",
    "    # 加载数据集\n",
    "    dataset = torch.load(dataset_path)\n",
    "\n",
    "    # 随机选择一个样本\n",
    "    sample_idx = random.randint(0, dataset.shape[0] - 1)  # 选择一个样本的索引\n",
    "    sample = dataset[sample_idx]  # 获取该样本\n",
    "\n",
    "    # 提取 E、N、Z 通道的数据\n",
    "    E_channel = sample[0].numpy()  # 第1通道\n",
    "    N_channel = sample[1].numpy()  # 第2通道\n",
    "    Z_channel = sample[2].numpy()  # 第3通道\n",
    "\n",
    "    # 创建一个图形，显示3个通道\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # E通道波形\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(E_channel, color='blue')\n",
    "    plt.title(f\"{title_prefix} {sample_idx} - E Channel\")\n",
    "    plt.xlabel('Sample Points')\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "    # N通道波形\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(N_channel, color='green')\n",
    "    plt.title(f\"{title_prefix} {sample_idx} - N Channel\")\n",
    "    plt.xlabel('Sample Points')\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "    # Z通道波形\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(Z_channel, color='red')\n",
    "    plt.title(f\"{title_prefix} {sample_idx} - Z Channel\")\n",
    "    plt.xlabel('Sample Points')\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "    # 调整布局，避免子图重叠\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化地震数据集\n",
    "load_and_plot(natural_dataset_path, title_prefix=\"Earthquake\")\n",
    "\n",
    "# 可视化爆炸数据集\n",
    "load_and_plot(ep_dataset_path, title_prefix=\"Explosion\")\n",
    "\n",
    "# 可视化塌陷数据集\n",
    "load_and_plot(ss_dataset_path, title_prefix=\"Collapse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#地震爆炸塌陷数据集情况\n",
    "import torch\n",
    "\n",
    "# 文件路径\n",
    "natural_dataset_path = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/tensor_dataset/natural_train_dataset.pt'  # 地震数据集\n",
    "ep_dataset_path = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/tensor_dataset/ep_train_dataset.pt'  # 爆炸数据集\n",
    "ss_dataset_path = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/tensor_dataset/ss_train_dataset.pt'  # 塌陷数据集\n",
    "\n",
    "# 打印数据集中的三分量数目\n",
    "def print_data_count(dataset_path, title_prefix=\"Sample\"):\n",
    "    # 加载数据集\n",
    "    dataset = torch.load(dataset_path)\n",
    "\n",
    "    # 获取数据集的大小，假设每个数据是3个通道（E, N, Z）\n",
    "    num_samples = dataset.shape[0]\n",
    "    print(f\"{title_prefix} dataset contains {num_samples} samples, each with 3 components (E, N, Z).\")\n",
    "\n",
    "# 打印地震数据集的三分量数量\n",
    "print_data_count(natural_dataset_path, title_prefix=\"Earthquake\")\n",
    "\n",
    "# 打印爆炸数据集的三分量数量\n",
    "print_data_count(ep_dataset_path, title_prefix=\"Explosion\")\n",
    "\n",
    "# 打印塌陷数据集的三分量数量\n",
    "print_data_count(ss_dataset_path, title_prefix=\"Collapse\")\n",
    "\n",
    "# 添加测试集数据集的数量\n",
    "test_natural_dataset_path = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/tensor_dataset/natural_test_dataset.pt' \n",
    "test_ep_dataset_path = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/tensor_dataset/ep_test_dataset.pt' \n",
    "test_ss_dataset_path = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/tensor_dataset/ss_test_dataset.pt'\n",
    "\n",
    "# 打印测试集的三分量数量\n",
    "print_data_count(test_natural_dataset_path, title_prefix=\"Test Earthquake\")\n",
    "print_data_count(test_ep_dataset_path, title_prefix=\"Test Explosion\")\n",
    "print_data_count(test_ss_dataset_path, title_prefix=\"Test Collapse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "folder = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_datasets/Earthquake_train_dataset\"\n",
    "\n",
    "# 找到所有 .npy\n",
    "files = sorted(glob.glob(os.path.join(folder, \"*.npy\")))\n",
    "print(f\"目录: {folder}\")\n",
    "print(f\"共找到 {len(files)} 个 .npy 文件\")\n",
    "\n",
    "if len(files) == 0:\n",
    "    raise RuntimeError(\"没找到任何 .npy 文件，请检查路径是否正确。\")\n",
    "\n",
    "# 随机抽样数量（你可改成 5）\n",
    "k = 3\n",
    "pick = random.sample(files, min(k, len(files)))\n",
    "\n",
    "# 打印设置：不省略（注意：如果数据非常大，仍可能被终端截断）\n",
    "np.set_printoptions(threshold=20000, linewidth=200, suppress=True)\n",
    "\n",
    "for fp in pick:\n",
    "    arr = np.load(fp)\n",
    "    fname = os.path.basename(fp)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"文件名: {fname}\")\n",
    "    print(f\"shape: {arr.shape}, dtype: {arr.dtype}\")\n",
    "\n",
    "    # 基本格式检查\n",
    "    if arr.ndim != 2 or arr.shape[1] != 3:\n",
    "        print(\"[警告] 数据格式不是 (L,3)，请确认保存格式是否正确。\")\n",
    "    \n",
    "    # 打印前10行\n",
    "    n = arr.shape[0] if arr.ndim == 2 else 0\n",
    "    head_n = min(10, n)\n",
    "    tail_n = min(10, n)\n",
    "\n",
    "    print(\"\\n前10个采样点 (每行[E,N,Z])：\")\n",
    "    print(arr[:head_n])\n",
    "\n",
    "    print(\"\\n后10个采样点 (每行[E,N,Z])：\")\n",
    "    print(arr[-tail_n:])\n",
    "\n",
    "    # 检测尾部是否有大量重复（判断是否存在“均值补齐”类似的平坦尾部）\n",
    "    if arr.ndim == 2 and n >= 50:\n",
    "        tail = arr[-200:] if n >= 200 else arr[-n:]\n",
    "        # 统计尾部每个通道“最后一个值”重复的长度\n",
    "        repeat_lens = []\n",
    "        for ch in range(3):\n",
    "            v = arr[-1, ch]\n",
    "            cnt = 0\n",
    "            # 从尾部往前数连续相等的长度（浮点数用近似判断）\n",
    "            for i in range(n-1, -1, -1):\n",
    "                if np.isclose(arr[i, ch], v, atol=1e-8, rtol=1e-6):\n",
    "                    cnt += 1\n",
    "                else:\n",
    "                    break\n",
    "            repeat_lens.append(cnt)\n",
    "\n",
    "        print(\"\\n尾部连续重复长度(按 E/N/Z 通道分别统计)：\")\n",
    "        print(f\"E: {repeat_lens[0]}  N: {repeat_lens[1]}  Z: {repeat_lens[2]}\")\n",
    "        if max(repeat_lens) > 100:\n",
    "            print(\"[提示] 尾部出现>100点连续相同，可能存在均值/常数填充或信号尾部平坦。\")\n",
    "\n",
    "print(\"\\n完成：已随机打印上述样本的关键内容。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转换数据存储格式\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "base_dir = '/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_datasets'\n",
    "\n",
    "folders = [\n",
    "    'Earthquake_train_dataset',\n",
    "    'Earthquake_test_dataset',\n",
    "    'Explosion_train_dataset',\n",
    "    'Explosion_test_dataset',\n",
    "    'Collapse_train_dataset',\n",
    "    'Collapse_test_dataset',\n",
    "]\n",
    "\n",
    "def load_npy_any(path: str):\n",
    "    \"\"\"\n",
    "    兼容两种保存方式：\n",
    "    1) 普通 ndarray: shape (3000,3) 或 (3,3000)\n",
    "    2) dict 保存成 npy: np.load -> shape=() object，需要 allow_pickle=True 然后 item()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        arr = np.load(path, allow_pickle=False)\n",
    "        # 普通数组直接返回\n",
    "        return arr\n",
    "    except ValueError:\n",
    "        # 可能是 object array（dict等），需要 allow_pickle=True\n",
    "        obj = np.load(path, allow_pickle=True)\n",
    "        # dict/对象通常是 0-d array，item() 取出来\n",
    "        if isinstance(obj, np.ndarray) and obj.shape == () and obj.dtype == object:\n",
    "            return obj.item()\n",
    "        return obj\n",
    "\n",
    "def to_channel_first(data):\n",
    "    \"\"\"\n",
    "    目标：返回 ndarray, shape = (3, 3000)；顺序为 [E, N, Z]\n",
    "    \"\"\"\n",
    "    # 情况A：data 是 dict {'E':..., 'N':..., 'Z':...}\n",
    "    if isinstance(data, dict):\n",
    "        if not all(k in data for k in ['E', 'N', 'Z']):\n",
    "            raise ValueError(\"dict 缺少 E/N/Z 键\")\n",
    "        E = np.asarray(data['E']).reshape(-1)\n",
    "        N = np.asarray(data['N']).reshape(-1)\n",
    "        Z = np.asarray(data['Z']).reshape(-1)\n",
    "        if not (len(E) == len(N) == len(Z) == 3000):\n",
    "            raise ValueError(f\"dict 中 E/N/Z 长度不为3000：E={len(E)}, N={len(N)}, Z={len(Z)}\")\n",
    "        return np.stack([E, N, Z], axis=0)\n",
    "\n",
    "    # 情况B：data 是 ndarray\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        raise ValueError(f\"未知数据类型：{type(data)}\")\n",
    "\n",
    "    if data.ndim != 2:\n",
    "        raise ValueError(f\"ndarray 维度不是2：shape={data.shape}\")\n",
    "\n",
    "    # (3000,3) -> (3,3000)\n",
    "    if data.shape == (3000, 3):\n",
    "        return data.T\n",
    "\n",
    "    # 已经是 (3,3000)\n",
    "    if data.shape == (3, 3000):\n",
    "        return data\n",
    "\n",
    "    # 兜底：如果是 (L,3) 也转置；如果是 (3,L) 也保留，但要检查 L=3000\n",
    "    if data.shape[1] == 3 and data.shape[0] == 3000:\n",
    "        return data.T\n",
    "    if data.shape[0] == 3 and data.shape[1] == 3000:\n",
    "        return data\n",
    "\n",
    "    raise ValueError(f\"不支持/不符合预期的shape：{data.shape}\")\n",
    "\n",
    "def safe_overwrite_npy(path: str, arr: np.ndarray):\n",
    "    \"\"\"\n",
    "    安全覆盖：先写临时文件，再 os.replace 原文件，避免中途写坏\n",
    "    \"\"\"\n",
    "    tmp_path = path + '.tmp.npy'\n",
    "    np.save(tmp_path, arr)\n",
    "    os.replace(tmp_path, path)\n",
    "\n",
    "# ------------------ 批量转换 + 记录日志 ------------------\n",
    "random.seed(42)\n",
    "\n",
    "for folder in folders:\n",
    "    dir_path = os.path.join(base_dir, folder)\n",
    "    if not os.path.isdir(dir_path):\n",
    "        print(f\"[跳过] 文件夹不存在: {dir_path}\")\n",
    "        continue\n",
    "\n",
    "    npy_files = [f for f in os.listdir(dir_path) if f.endswith('.npy')]\n",
    "    npy_files.sort()\n",
    "\n",
    "    converted = 0\n",
    "    already_ok = 0\n",
    "    failed = []\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"开始处理文件夹: {dir_path}\")\n",
    "    print(f\"共 {len(npy_files)} 个 .npy\")\n",
    "\n",
    "    for fname in npy_files:\n",
    "        fpath = os.path.join(dir_path, fname)\n",
    "        try:\n",
    "            data = load_npy_any(fpath)\n",
    "            new_arr = to_channel_first(data)\n",
    "\n",
    "            # 如果已经是 (3,3000) 且内容就是 ndarray，就算 already_ok\n",
    "            # 但不深度比较内容了，仅判断 shape\n",
    "            if isinstance(data, np.ndarray) and data.shape == (3, 3000):\n",
    "                already_ok += 1\n",
    "                # 仍然可以选择覆盖/不覆盖。这里不覆盖，减少IO\n",
    "                continue\n",
    "\n",
    "            safe_overwrite_npy(fpath, new_arr)\n",
    "            converted += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            failed.append((fname, str(e)))\n",
    "\n",
    "    print(f\"[完成] 转换覆盖: {converted} 个 | 已符合(3,3000)未改动: {already_ok} 个 | 失败: {len(failed)} 个\")\n",
    "    if failed:\n",
    "        print(\"---- 失败文件列表(最多显示20个) ----\")\n",
    "        for item in failed[:20]:\n",
    "            print(f\"  {item[0]} -> {item[1]}\")\n",
    "\n",
    "    # ------------------ 随机抽样打印检查 ------------------\n",
    "    print(\"\\n[抽样检查] 随机打印 3 个文件（展示 shape + 各通道前10点/后10点）\")\n",
    "    if len(npy_files) == 0:\n",
    "        continue\n",
    "\n",
    "    sample_k = min(3, len(npy_files))\n",
    "    sample_files = random.sample(npy_files, sample_k)\n",
    "    for sf in sample_files:\n",
    "        sp = os.path.join(dir_path, sf)\n",
    "        arr = np.load(sp, allow_pickle=False)\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(f\"文件: {sf}\")\n",
    "        print(f\"shape: {arr.shape}, dtype: {arr.dtype}\")\n",
    "        if arr.shape != (3, 3000):\n",
    "            print(\"  [警告] shape不是(3,3000)，请检查！\")\n",
    "            continue\n",
    "\n",
    "        E, N, Z = arr[0], arr[1], arr[2]\n",
    "        print(\"E通道 前10点:\", E[:10])\n",
    "        print(\"E通道 后10点:\", E[-10:])\n",
    "        print(\"N通道 前10点:\", N[:10])\n",
    "        print(\"N通道 后10点:\", N[-10:])\n",
    "        print(\"Z通道 前10点:\", Z[:10])\n",
    "        print(\"Z通道 后10点:\", Z[-10:])\n",
    "\n",
    "print(\"\\n全部文件夹处理结束。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证集划分\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "random.seed(42)  # 固定随机种子，保证可复现（你也可以改成 None 或删除）\n",
    "\n",
    "BASE = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_datasets\")\n",
    "\n",
    "# 你现在应该有这 9 个文件夹（train/test 已存在，valid 将创建）\n",
    "FOLDERS = {\n",
    "    \"Earthquake\": {\n",
    "        \"train\": BASE / \"Earthquake_train_dataset\",\n",
    "        \"test\":  BASE / \"Earthquake_test_dataset\",\n",
    "        \"valid\": BASE / \"Earthquake_valid_dataset\",\n",
    "    },\n",
    "    \"Explosion\": {\n",
    "        \"train\": BASE / \"Explosion_train_dataset\",\n",
    "        \"test\":  BASE / \"Explosion_test_dataset\",\n",
    "        \"valid\": BASE / \"Explosion_valid_dataset\",   # ✅按类别正确建立\n",
    "    },\n",
    "    \"Collapse\": {\n",
    "        \"train\": BASE / \"Collapse_train_dataset\",\n",
    "        \"test\":  BASE / \"Collapse_test_dataset\",\n",
    "        \"valid\": BASE / \"Collapse_valid_dataset\",     # ✅按正确拼写建立\n",
    "        # 兼容你写错的 Collpase_valid_dataset（如果你之前已经建了错名文件夹，这里也能处理）\n",
    "        \"valid_typo\": BASE / \"Collpase_valid_dataset\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def list_npy_files(folder: Path):\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    return sorted([p for p in folder.iterdir() if p.is_file() and p.suffix == \".npy\"])\n",
    "\n",
    "def ensure_dir(folder: Path):\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def move_random_percent(src: Path, dst: Path, percent: float):\n",
    "    \"\"\"\n",
    "    从 src 随机移动 percent 比例的 .npy 到 dst\n",
    "    \"\"\"\n",
    "    ensure_dir(dst)\n",
    "\n",
    "    files = list_npy_files(src)\n",
    "    n = len(files)\n",
    "    if n == 0:\n",
    "        print(f\"[WARN] {src} 中没有 .npy 文件，跳过。\")\n",
    "        return 0\n",
    "\n",
    "    k = int(math.floor(n * percent))\n",
    "    if k <= 0:\n",
    "        print(f\"[WARN] {src} 文件数={n}，按 {percent*100:.0f}% 计算要移动 {k} 个，跳过。\")\n",
    "        return 0\n",
    "\n",
    "    chosen = random.sample(files, k)\n",
    "\n",
    "    print(f\"\\n[INFO] 从 {src} 随机抽取 {k}/{n} ({percent*100:.0f}%) 个文件移动到 {dst}\")\n",
    "    moved = 0\n",
    "    for p in chosen:\n",
    "        target = dst / p.name\n",
    "        try:\n",
    "            shutil.move(str(p), str(target))\n",
    "            moved += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 移动失败: {p} -> {target} | {e}\")\n",
    "\n",
    "    print(f\"[OK] 实际移动成功 {moved} 个\")\n",
    "    return moved\n",
    "\n",
    "def safe_rename_all(folder: Path, prefix: str):\n",
    "    \"\"\"\n",
    "    将 folder 下所有 .npy 重命名为 prefix_00001.npy ... 顺序稳定（按原文件名排序）\n",
    "    为避免重名冲突，采用“两阶段重命名”：\n",
    "      1) 先改成 __tmp__xxxx.npy\n",
    "      2) 再改成最终名字\n",
    "    \"\"\"\n",
    "    files = list_npy_files(folder)\n",
    "    n = len(files)\n",
    "    if n == 0:\n",
    "        print(f\"[WARN] {folder} 没有 .npy 文件，跳过重命名。\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n[INFO] 重命名 {folder} 中 {n} 个文件 -> {prefix}_00001.npy ...\")\n",
    "\n",
    "    # 1) 临时名\n",
    "    tmp_names = []\n",
    "    for i, p in enumerate(files, start=1):\n",
    "        tmp = folder / f\"__tmp__{i:08d}.npy\"\n",
    "        try:\n",
    "            p.rename(tmp)\n",
    "            tmp_names.append(tmp)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 临时重命名失败: {p} -> {tmp} | {e}\")\n",
    "            return\n",
    "\n",
    "    # 2) 最终名\n",
    "    for i, tmp in enumerate(tmp_names, start=1):\n",
    "        final = folder / f\"{prefix}_{i:05d}.npy\"\n",
    "        try:\n",
    "            tmp.rename(final)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 最终重命名失败: {tmp} -> {final} | {e}\")\n",
    "            return\n",
    "\n",
    "    print(f\"[OK] 重命名完成：{folder}\")\n",
    "\n",
    "def count_all_9_folders():\n",
    "    print(\"\\n================= 当前 9 个子文件夹样本数量统计 =================\")\n",
    "    total = 0\n",
    "    for cls, paths in FOLDERS.items():\n",
    "        for split in [\"train\", \"valid\", \"test\"]:\n",
    "            folder = paths.get(split)\n",
    "            if folder is None:\n",
    "                continue\n",
    "            c = len(list_npy_files(folder))\n",
    "            print(f\"{folder}: {c}\")\n",
    "            total += c\n",
    "    # 如果你之前已经建了 typo 的 valid 文件夹，也给你统计一下\n",
    "    typo = FOLDERS[\"Collapse\"].get(\"valid_typo\")\n",
    "    if typo and typo.exists():\n",
    "        c = len(list_npy_files(typo))\n",
    "        print(f\"{typo} (typo folder): {c}\")\n",
    "    print(\"================================================================\")\n",
    "    print(f\"[TOTAL] 以上统计总数（train+valid+test，不含空缺项）= {total}\\n\")\n",
    "\n",
    "def main():\n",
    "    # 0) 创建 valid 文件夹\n",
    "    ensure_dir(FOLDERS[\"Earthquake\"][\"valid\"])\n",
    "    ensure_dir(FOLDERS[\"Explosion\"][\"valid\"])\n",
    "    ensure_dir(FOLDERS[\"Collapse\"][\"valid\"])\n",
    "\n",
    "    # 如果你之前误建了 Collpase_valid_dataset，并且里面有东西，可选择合并到正确的 Collapse_valid_dataset\n",
    "    typo_valid = FOLDERS[\"Collapse\"].get(\"valid_typo\")\n",
    "    if typo_valid and typo_valid.exists():\n",
    "        typo_files = list_npy_files(typo_valid)\n",
    "        if len(typo_files) > 0:\n",
    "            print(f\"[WARN] 检测到错拼文件夹 {typo_valid}，里面有 {len(typo_files)} 个文件。\")\n",
    "            print(\"      将其文件全部移动合并到正确的 Collapse_valid_dataset ...\")\n",
    "            for p in typo_files:\n",
    "                try:\n",
    "                    shutil.move(str(p), str(FOLDERS[\"Collapse\"][\"valid\"] / p.name))\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] 合并移动失败: {p} | {e}\")\n",
    "            # 如果空了可以删（可选）\n",
    "            try:\n",
    "                if len(list(typo_valid.iterdir())) == 0:\n",
    "                    typo_valid.rmdir()\n",
    "                    print(f\"[OK] 已删除空的错拼文件夹: {typo_valid}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 1) 各类别 train 抽 20% 移动到 valid\n",
    "    move_random_percent(FOLDERS[\"Earthquake\"][\"train\"], FOLDERS[\"Earthquake\"][\"valid\"], 0.20)\n",
    "    move_random_percent(FOLDERS[\"Explosion\"][\"train\"],  FOLDERS[\"Explosion\"][\"valid\"],  0.20)\n",
    "    move_random_percent(FOLDERS[\"Collapse\"][\"train\"],   FOLDERS[\"Collapse\"][\"valid\"],   0.20)\n",
    "\n",
    "    # 2) 对 9 个文件夹全部重命名排序\n",
    "    # 规则：{Class}_{split}_00001.npy\n",
    "    for cls, paths in FOLDERS.items():\n",
    "        for split in [\"train\", \"valid\", \"test\"]:\n",
    "            folder = paths.get(split)\n",
    "            if folder is None:\n",
    "                continue\n",
    "            if not folder.exists():\n",
    "                print(f\"[WARN] 文件夹不存在，跳过：{folder}\")\n",
    "                continue\n",
    "            prefix = f\"{cls}_{split}\"\n",
    "            safe_rename_all(folder, prefix)\n",
    "\n",
    "    # 3) 统计 9 个子文件夹数量\n",
    "    count_all_9_folders()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Union\n",
    "\n",
    "# =========================\n",
    "# 配置区：按你的路径写死\n",
    "# =========================\n",
    "BASE_IN = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_datasets\")\n",
    "BASE_OUT = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_datasets_enhancement\")\n",
    "\n",
    "CONFIG = [\n",
    "    {\n",
    "        \"name\": \"Explosion\",\n",
    "        \"train\": BASE_IN / \"Explosion_train_dataset\",\n",
    "        \"valid\": BASE_IN / \"Explosion_valid_dataset\",\n",
    "        \"test\":  BASE_IN / \"Explosion_test_dataset\",\n",
    "        \"target_trainvalid_total\": 44000,\n",
    "        \"target_test_total\": 11230,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Collapse\",\n",
    "        \"train\": BASE_IN / \"Collapse_train_dataset\",\n",
    "        \"valid\": BASE_IN / \"Collapse_valid_dataset\",\n",
    "        \"test\":  BASE_IN / \"Collapse_test_dataset\",\n",
    "        \"target_trainvalid_total\": 42490,\n",
    "        \"target_test_total\": 9950,\n",
    "    },\n",
    "]\n",
    "\n",
    "# 统一输出长度\n",
    "SEG_LEN = 3000\n",
    "# 起点范围：前 200 个采样点内\n",
    "SHIFT_RANGE = 200\n",
    "# 每条波形最多生成 10 个 shift 片段\n",
    "MAX_SHIFTS_PER_SAMPLE = 10\n",
    "\n",
    "# 随机种子：保证可复现（你想完全随机可改为 None）\n",
    "SEED = 20260108\n",
    "\n",
    "# 输出格式：你喜欢按通道存储 -> (3, 3000)\n",
    "OUTPUT_CHANNEL_FIRST = True\n",
    "\n",
    "# 每处理多少条打印一次进度（避免刷屏）\n",
    "PROGRESS_EVERY = 200\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 工具函数\n",
    "# =========================\n",
    "def list_npy_files(folder: Path):\n",
    "    return sorted([p for p in folder.iterdir() if p.is_file() and p.suffix == \".npy\"])\n",
    "\n",
    "\n",
    "def ensure_empty_dir(folder: Path):\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    # 清空旧的 .npy（只清 enhancement 目录，安全）\n",
    "    old = list_npy_files(folder)\n",
    "    if old:\n",
    "        for p in old:\n",
    "            p.unlink()\n",
    "        print(f\"[INFO] 已清空输出目录旧文件: {folder} (removed {len(old)} .npy)\")\n",
    "\n",
    "\n",
    "def load_waveform_any(path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    支持读取：\n",
    "    - ndarray: (3000,3) 或 (3,3000) 或 (T,3)/(3,T)\n",
    "    - dict: {'E','N','Z'}（以防还有旧格式）\n",
    "    返回统一格式：time-major -> (T,3)\n",
    "    \"\"\"\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "\n",
    "    # 处理 dict（可能是 np.ndarray(object) 包一层）\n",
    "    if isinstance(data, np.ndarray) and data.shape == () and data.dtype == object:\n",
    "        data = data.item()\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        E = np.asarray(data[\"E\"])\n",
    "        N = np.asarray(data[\"N\"])\n",
    "        Z = np.asarray(data[\"Z\"])\n",
    "        x = np.column_stack([E, N, Z])  # (T,3)\n",
    "        return x.astype(np.float32)\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        x = data\n",
    "        if x.ndim != 2:\n",
    "            raise ValueError(f\"Unexpected ndim={x.ndim} in {path}\")\n",
    "\n",
    "        # (T,3) 或 (3,T)\n",
    "        if x.shape[1] == 3:\n",
    "            return x.astype(np.float32)  # (T,3)\n",
    "        if x.shape[0] == 3:\n",
    "            return x.T.astype(np.float32)  # -> (T,3)\n",
    "\n",
    "        raise ValueError(f\"Unexpected shape={x.shape} in {path}\")\n",
    "\n",
    "    raise ValueError(f\"Unsupported data type in {path}: {type(data)}\")\n",
    "\n",
    "\n",
    "def safe_mean_tail(x_tail: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x_tail: (t,3)\n",
    "    计算每通道均值（忽略 NaN），若全 NaN 则置 0\n",
    "    \"\"\"\n",
    "    m = np.nanmean(x_tail, axis=0)\n",
    "    m = np.where(np.isfinite(m), m, 0.0)\n",
    "    return m.astype(np.float32)\n",
    "\n",
    "\n",
    "def extract_segment(x: np.ndarray, start: int, seg_len: int = SEG_LEN) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x: (T,3)\n",
    "    start: 起点\n",
    "    返回: (seg_len,3)，不足补齐（按“剩余波形均值”补尾部）\n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "    start = int(start)\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "    if start >= T:\n",
    "        # 理论不会发生（我们会限制 start），但保险\n",
    "        tail_mean = safe_mean_tail(x[-1:, :]) if T > 0 else np.zeros(3, dtype=np.float32)\n",
    "        seg = np.tile(tail_mean, (seg_len, 1))\n",
    "        return seg\n",
    "\n",
    "    seg = x[start:start + seg_len, :]\n",
    "    if seg.shape[0] < seg_len:\n",
    "        tail = x[start:, :]\n",
    "        tail_mean = safe_mean_tail(tail) if tail.shape[0] > 0 else np.zeros(3, dtype=np.float32)\n",
    "        pad = np.tile(tail_mean, (seg_len - seg.shape[0], 1))\n",
    "        seg = np.vstack([seg, pad])\n",
    "\n",
    "    return seg.astype(np.float32)\n",
    "\n",
    "\n",
    "def to_output_format(seg_time_major: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    输入: (3000,3)\n",
    "    输出: (3,3000) 或 (3000,3)\n",
    "    \"\"\"\n",
    "    if OUTPUT_CHANNEL_FIRST:\n",
    "        return seg_time_major.T  # (3,3000)\n",
    "    return seg_time_major       # (3000,3)\n",
    "\n",
    "\n",
    "def save_npy(out_dir: Path, prefix: str, idx: int, arr: np.ndarray):\n",
    "    out_path = out_dir / f\"{prefix}_{idx:05d}.npy\"\n",
    "    np.save(out_path, arr)\n",
    "\n",
    "\n",
    "def compute_targets_split(n_train: int, n_valid: int, target_total: int) -> Tuple[int, int, float]:\n",
    "    \"\"\"\n",
    "    让 train/valid 保持相同倍率（尽量），并保证两者和严格等于 target_total\n",
    "    \"\"\"\n",
    "    total = n_train + n_valid\n",
    "    m = target_total / total\n",
    "    train_target = int(round(n_train * m))\n",
    "    # 修正确保和一致\n",
    "    train_target = max(train_target, n_train)\n",
    "    valid_target = target_total - train_target\n",
    "    if valid_target < n_valid:\n",
    "        valid_target = n_valid\n",
    "        train_target = target_total - valid_target\n",
    "    return train_target, valid_target, m\n",
    "\n",
    "\n",
    "def augment_folder_to_target(\n",
    "    in_dir: Path,\n",
    "    out_dir: Path,\n",
    "    out_prefix: str,\n",
    "    target_total: int,\n",
    "    rng: np.random.Generator\n",
    "):\n",
    "    files = list_npy_files(in_dir)\n",
    "    n = len(files)\n",
    "    if n == 0:\n",
    "        raise RuntimeError(f\"Input folder empty: {in_dir}\")\n",
    "\n",
    "    need_aug = target_total - n\n",
    "    if need_aug < 0:\n",
    "        raise RuntimeError(f\"target_total({target_total}) < original({n}) for {in_dir}\")\n",
    "\n",
    "    base = need_aug // n\n",
    "    rem = need_aug % n\n",
    "\n",
    "    if base > MAX_SHIFTS_PER_SAMPLE:\n",
    "        raise RuntimeError(\n",
    "            f\"Need too many augmentations per sample: base={base} > {MAX_SHIFTS_PER_SAMPLE}. \"\n",
    "            f\"original={n}, target={target_total}\"\n",
    "        )\n",
    "\n",
    "    ensure_empty_dir(out_dir)\n",
    "\n",
    "    print(f\"\\n[INFO] 输入目录: {in_dir}\")\n",
    "    print(f\"[INFO] 输出目录: {out_dir}\")\n",
    "    print(f\"[INFO] 原始样本数: {n}\")\n",
    "    print(f\"[INFO] 目标样本数: {target_total}\")\n",
    "    print(f\"[INFO] 需要新增(增强)样本数: {need_aug}\")\n",
    "    print(f\"[INFO] 增强分配策略: 每条基础增强 {base} 次，另外 {rem} 条再 +1 次\")\n",
    "    print(f\"[INFO] 每条最多候选 shift={MAX_SHIFTS_PER_SAMPLE}，shift 起点范围=[0,{SHIFT_RANGE-1}]\")\n",
    "\n",
    "    out_count = 0\n",
    "\n",
    "    for i, fp in enumerate(files):\n",
    "        x = load_waveform_any(fp)  # (T,3)\n",
    "        T = x.shape[0]\n",
    "\n",
    "        # 1) 先保存基础片段 start=0\n",
    "        base_seg = extract_segment(x, start=0, seg_len=SEG_LEN)\n",
    "        out_arr = to_output_format(base_seg)\n",
    "        out_count += 1\n",
    "        save_npy(out_dir, out_prefix, out_count, out_arr)\n",
    "\n",
    "        # 2) 决定这条需要增强几次\n",
    "        n_aug_i = base + (1 if i < rem else 0)\n",
    "        if n_aug_i > 0:\n",
    "            max_start = min(SHIFT_RANGE, T) if T > 0 else 1\n",
    "            if max_start >= MAX_SHIFTS_PER_SAMPLE:\n",
    "                starts = rng.choice(np.arange(max_start), size=MAX_SHIFTS_PER_SAMPLE, replace=False)\n",
    "            else:\n",
    "                # 长度不足 200 或不足 10 个起点时：退化策略（允许重复）\n",
    "                starts = rng.choice(np.arange(max_start), size=MAX_SHIFTS_PER_SAMPLE, replace=True)\n",
    "\n",
    "            # 取前 n_aug_i 个 start\n",
    "            for k in range(n_aug_i):\n",
    "                s = int(starts[k])\n",
    "                seg = extract_segment(x, start=s, seg_len=SEG_LEN)\n",
    "                out_arr = to_output_format(seg)\n",
    "                out_count += 1\n",
    "                save_npy(out_dir, out_prefix, out_count, out_arr)\n",
    "\n",
    "        if (i + 1) % PROGRESS_EVERY == 0 or (i + 1) == n:\n",
    "            print(f\"[PROGRESS] {out_prefix}: processed {i+1}/{n}, saved {out_count}/{target_total}\")\n",
    "\n",
    "    # 校验\n",
    "    final_files = list_npy_files(out_dir)\n",
    "    if len(final_files) != target_total:\n",
    "        print(f\"[WARN] 输出数量不匹配: got={len(final_files)} expected={target_total}\")\n",
    "    else:\n",
    "        print(f\"[OK] 输出数量校验通过: {len(final_files)} files\")\n",
    "\n",
    "    return n, target_total\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 主流程\n",
    "# =========================\n",
    "def main():\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[INFO] Enhancement root: {BASE_OUT}\")\n",
    "    print(f\"[INFO] OUTPUT_CHANNEL_FIRST={OUTPUT_CHANNEL_FIRST} -> 保存 shape={(3,SEG_LEN) if OUTPUT_CHANNEL_FIRST else (SEG_LEN,3)}\")\n",
    "    print(f\"[INFO] RNG seed={SEED}\\n\")\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for item in CONFIG:\n",
    "        name = item[\"name\"]\n",
    "\n",
    "        train_in = item[\"train\"]\n",
    "        valid_in = item[\"valid\"]\n",
    "        test_in  = item[\"test\"]\n",
    "\n",
    "        train_files = list_npy_files(train_in)\n",
    "        valid_files = list_npy_files(valid_in)\n",
    "        test_files  = list_npy_files(test_in)\n",
    "\n",
    "        n_train = len(train_files)\n",
    "        n_valid = len(valid_files)\n",
    "        n_test  = len(test_files)\n",
    "\n",
    "        # 计算 train/valid 的目标拆分（保持同倍率）\n",
    "        tv_target = item[\"target_trainvalid_total\"]\n",
    "        train_target, valid_target, m = compute_targets_split(n_train, n_valid, tv_target)\n",
    "\n",
    "        print(\"============================================================\")\n",
    "        print(f\"[DATASET] {name}\")\n",
    "        print(f\"[INFO] 原始 train={n_train}, valid={n_valid}, train+valid={n_train+n_valid}\")\n",
    "        print(f\"[INFO] 目标 train+valid 总数={tv_target}\")\n",
    "        print(f\"[INFO] -> 目标拆分: train={train_target}, valid={valid_target} (倍率 m≈{m:.4f}, 增强≈{(m-1)*100:.2f}%)\")\n",
    "        print(f\"[INFO] 原始 test={n_test}, 目标 test={item['target_test_total']} (倍率≈{item['target_test_total']/max(n_test,1):.4f})\")\n",
    "\n",
    "        # 输出目录\n",
    "        train_out = BASE_OUT / f\"{name}_train_dataset_enhancement\"\n",
    "        valid_out = BASE_OUT / f\"{name}_valid_dataset_enhancement\"\n",
    "        test_out  = BASE_OUT / f\"{name}_test_dataset_enhancement\"\n",
    "\n",
    "        # 生成增强数据集（包含原始复制 + 增强）\n",
    "        orig_train, final_train = augment_folder_to_target(\n",
    "            train_in, train_out, train_out.name, train_target, rng\n",
    "        )\n",
    "        orig_valid, final_valid = augment_folder_to_target(\n",
    "            valid_in, valid_out, valid_out.name, valid_target, rng\n",
    "        )\n",
    "        orig_test, final_test = augment_folder_to_target(\n",
    "            test_in, test_out, test_out.name, item[\"target_test_total\"], rng\n",
    "        )\n",
    "\n",
    "        summary.append((train_out, orig_train, final_train))\n",
    "        summary.append((valid_out, orig_valid, final_valid))\n",
    "        summary.append((test_out,  orig_test,  final_test))\n",
    "\n",
    "    # 汇总统计\n",
    "    print(\"\\n==================== 最终输出目录统计 ====================\")\n",
    "    total_all = 0\n",
    "    for folder, orig, final in summary:\n",
    "        cnt = len(list_npy_files(folder))\n",
    "        total_all += cnt\n",
    "        ratio = (cnt / orig) if orig > 0 else float(\"inf\")\n",
    "        print(f\"{folder}: files={cnt} | original={orig} | final={final} | 实际倍率≈{ratio:.4f}\")\n",
    "\n",
    "    print(\"=========================================================\")\n",
    "    print(f\"[TOTAL] enhancement 总文件数 = {total_all}\")\n",
    "    print(\"[DONE] 全部增强完成（原始数据集未移动/未删除，仅在 enhancement 目录生成新数据集）\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# ======================\n",
    "# 路径配置\n",
    "# ======================\n",
    "SRC_DIR = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/DiTing2.0_Noise_dataset_step4\")\n",
    "DST_ROOT = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\")\n",
    "\n",
    "# 期望数量（用于核对）\n",
    "EXPECTED = {\n",
    "    \"Junior_A\": 18611,\n",
    "    \"Expert_A\": 44026,\n",
    "    \"Expert_B\": 40650,\n",
    "    \"Expert_C\": 49849,\n",
    "}\n",
    "\n",
    "# 输出文件夹名映射\n",
    "OUT_DIRNAME = {\n",
    "    \"Junior_A\": \"Noise_Junior_A\",\n",
    "    \"Expert_A\": \"Noise_Expert_A\",\n",
    "    \"Expert_B\": \"Noise_Expert_B\",\n",
    "    \"Expert_C\": \"Noise_Expert_C\",\n",
    "}\n",
    "\n",
    "# 文件名匹配：例如 0_Expert_A_Noise_step4.npy\n",
    "PATTERN = re.compile(r\"^(?P<id>\\d+)_(?P<expert>Junior_A|Expert_A|Expert_B|Expert_C)_Noise_step4\\.npy$\")\n",
    "\n",
    "# 是否覆盖已存在输出目录里的 npy（True 会先清空目标目录）\n",
    "CLEAR_DST_DIRS = True\n",
    "\n",
    "# ======================\n",
    "# 工具函数\n",
    "# ======================\n",
    "def list_npy_files(folder: Path):\n",
    "    return sorted([p for p in folder.iterdir() if p.is_file() and p.suffix == \".npy\"])\n",
    "\n",
    "def ensure_dir(folder: Path):\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def clear_npy_in_dir(folder: Path):\n",
    "    if not folder.exists():\n",
    "        return 0\n",
    "    cnt = 0\n",
    "    for p in folder.iterdir():\n",
    "        if p.is_file() and p.suffix == \".npy\":\n",
    "            p.unlink()\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "# ======================\n",
    "# 主逻辑\n",
    "# ======================\n",
    "def main():\n",
    "    if not SRC_DIR.exists():\n",
    "        raise FileNotFoundError(f\"[ERROR] 源目录不存在: {SRC_DIR}\")\n",
    "\n",
    "    ensure_dir(DST_ROOT)\n",
    "\n",
    "    # 创建/清空目标目录\n",
    "    dst_dirs = {}\n",
    "    for expert, dname in OUT_DIRNAME.items():\n",
    "        d = DST_ROOT / dname\n",
    "        ensure_dir(d)\n",
    "        if CLEAR_DST_DIRS:\n",
    "            removed = clear_npy_in_dir(d)\n",
    "            if removed > 0:\n",
    "                print(f\"[INFO] 已清空 {d} 旧的 .npy 文件: {removed} 个\")\n",
    "        dst_dirs[expert] = d\n",
    "\n",
    "    # 扫描源文件\n",
    "    all_files = list_npy_files(SRC_DIR)\n",
    "    print(f\"[INFO] 源目录: {SRC_DIR}\")\n",
    "    print(f\"[INFO] 共扫描到 .npy 文件: {len(all_files)} 个\")\n",
    "\n",
    "    # 分组收集\n",
    "    groups = {k: [] for k in OUT_DIRNAME.keys()}\n",
    "    unmatched = []\n",
    "\n",
    "    for fp in all_files:\n",
    "        m = PATTERN.match(fp.name)\n",
    "        if not m:\n",
    "            unmatched.append(fp.name)\n",
    "            continue\n",
    "        expert = m.group(\"expert\")\n",
    "        groups[expert].append(fp)\n",
    "\n",
    "    # 打印匹配情况\n",
    "    for expert in [\"Junior_A\", \"Expert_A\", \"Expert_B\", \"Expert_C\"]:\n",
    "        print(f\"[INFO] 匹配到 {expert}: {len(groups[expert])} 个文件\")\n",
    "\n",
    "    if unmatched:\n",
    "        print(f\"[WARN] 有 {len(unmatched)} 个文件名不符合规则，已跳过。示例前10个：\")\n",
    "        for x in unmatched[:10]:\n",
    "            print(\"   -\", x)\n",
    "\n",
    "    # 复制 + 重命名\n",
    "    total_copied = 0\n",
    "    for expert in [\"Junior_A\", \"Expert_A\", \"Expert_B\", \"Expert_C\"]:\n",
    "        src_list = sorted(groups[expert], key=lambda p: p.name)\n",
    "        dst_dir = dst_dirs[expert]\n",
    "        prefix = OUT_DIRNAME[expert]  # Noise_Expert_A 这种\n",
    "\n",
    "        print(f\"\\n[INFO] 开始处理 {expert} -> {dst_dir}\")\n",
    "        for i, src_path in enumerate(src_list, start=1):\n",
    "            new_name = f\"{prefix}_{i:05d}.npy\"\n",
    "            dst_path = dst_dir / new_name\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            total_copied += 1\n",
    "\n",
    "            # 适度打印进度，避免刷屏\n",
    "            if i % 2000 == 0 or i == len(src_list):\n",
    "                print(f\"[PROGRESS] {prefix}: {i}/{len(src_list)} 已复制并重命名\")\n",
    "\n",
    "        print(f\"[OK] {expert} 完成：输出 {len(src_list)} 个文件\")\n",
    "\n",
    "    # 最终统计 + 核对\n",
    "    print(\"\\n================ 最终输出统计 ================\")\n",
    "    for expert in [\"Junior_A\", \"Expert_A\", \"Expert_B\", \"Expert_C\"]:\n",
    "        out_dir = dst_dirs[expert]\n",
    "        cnt = len(list_npy_files(out_dir))\n",
    "        exp = EXPECTED.get(expert, None)\n",
    "        if exp is not None and cnt != exp:\n",
    "            print(f\"[CHECK] {OUT_DIRNAME[expert]}: {cnt} 个 (期望 {exp})  <-- 数量不一致\")\n",
    "        else:\n",
    "            print(f\"[CHECK] {OUT_DIRNAME[expert]}: {cnt} 个 (期望 {exp})\")\n",
    "\n",
    "    print(\"==============================================\")\n",
    "    print(f\"[DONE] 总复制输出文件数: {total_copied}\")\n",
    "    print(f\"[DONE] 输出根目录: {DST_ROOT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# 配置\n",
    "# =========================\n",
    "ROOT = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\")\n",
    "\n",
    "BASES = [\n",
    "    \"Noise_Expert_A\",\n",
    "    \"Noise_Expert_B\",\n",
    "    \"Noise_Expert_C\",\n",
    "    \"Noise_Junior_A\",\n",
    "]\n",
    "\n",
    "TEST_RATIO = 0.20     # 第一次：train -> test\n",
    "VALID_RATIO = 0.20    # 第二次：剩余train -> valid\n",
    "\n",
    "RANDOM_SEED = 42      # 固定随机种子，便于复现（想完全随机可改成 None）\n",
    "\n",
    "# =========================\n",
    "# 工具函数\n",
    "# =========================\n",
    "def list_npy_files(folder: Path):\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    return sorted([p for p in folder.iterdir() if p.is_file() and p.suffix == \".npy\"])\n",
    "\n",
    "def ensure_dir(folder: Path):\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def move_random_subset(src_dir: Path, dst_dir: Path, ratio: float, rng: random.Random, stage_name: str):\n",
    "    files = list_npy_files(src_dir)\n",
    "    total = len(files)\n",
    "    if total == 0:\n",
    "        print(f\"[WARN] {stage_name}: 源目录为空：{src_dir}\")\n",
    "        return 0, 0\n",
    "\n",
    "    n_move = int(total * ratio)\n",
    "    # 如果 total>0 且 ratio>0，但算出来 0，则至少移动 1 个（避免比例太小导致不动）\n",
    "    if ratio > 0 and total > 0 and n_move == 0:\n",
    "        n_move = 1\n",
    "\n",
    "    chosen = rng.sample(files, n_move) if n_move > 0 else []\n",
    "\n",
    "    ensure_dir(dst_dir)\n",
    "\n",
    "    # 目标目录非空警告（不删除，继续做；最后会统一重命名）\n",
    "    dst_exist = len(list_npy_files(dst_dir))\n",
    "    if dst_exist > 0:\n",
    "        print(f\"[WARN] {stage_name}: 目标目录已存在 {dst_exist} 个 .npy，仍将继续移动并在最后统一重命名：{dst_dir}\")\n",
    "\n",
    "    moved = 0\n",
    "    for fp in chosen:\n",
    "        dst_path = dst_dir / fp.name\n",
    "        # 为避免同名冲突：若存在，则加后缀\n",
    "        if dst_path.exists():\n",
    "            dst_path = dst_dir / f\"__moved__{fp.stem}__{rng.randint(0, 10**9)}.npy\"\n",
    "        shutil.move(str(fp), str(dst_path))\n",
    "        moved += 1\n",
    "\n",
    "    remain = len(list_npy_files(src_dir))\n",
    "    print(f\"[INFO] {stage_name}: 从 {src_dir} 随机移动 {moved}/{total} ({ratio*100:.0f}%) -> {dst_dir} | 现在源目录剩余 {remain}\")\n",
    "    return moved, remain\n",
    "\n",
    "def safe_rename_all(folder: Path, prefix: str):\n",
    "    \"\"\"\n",
    "    把 folder 内所有 .npy 重命名为 prefix_00001.npy ...（先临时改名避免冲突）\n",
    "    \"\"\"\n",
    "    files = list_npy_files(folder)\n",
    "    n = len(files)\n",
    "    if n == 0:\n",
    "        print(f\"[INFO] 重命名跳过（空文件夹）：{folder}\")\n",
    "        return 0\n",
    "\n",
    "    # 第一步：临时命名\n",
    "    tmp_paths = []\n",
    "    for i, fp in enumerate(files, start=1):\n",
    "        tmp_name = f\"__tmp__{i:06d}__.npy\"\n",
    "        tmp_path = folder / tmp_name\n",
    "        shutil.move(str(fp), str(tmp_path))\n",
    "        tmp_paths.append(tmp_path)\n",
    "\n",
    "    # 第二步：最终命名\n",
    "    tmp_paths = sorted(tmp_paths, key=lambda p: p.name)\n",
    "    for i, fp in enumerate(tmp_paths, start=1):\n",
    "        final_name = f\"{prefix}_{i:05d}.npy\"\n",
    "        final_path = folder / final_name\n",
    "        shutil.move(str(fp), str(final_path))\n",
    "\n",
    "    print(f\"[OK] 重命名完成：{folder} | 共 {n} 个 -> {prefix}_00001.npy ...\")\n",
    "    return n\n",
    "\n",
    "def count_folder(folder: Path):\n",
    "    return len(list_npy_files(folder))\n",
    "\n",
    "# =========================\n",
    "# 主流程\n",
    "# =========================\n",
    "def main():\n",
    "    rng = random.Random(RANDOM_SEED) if RANDOM_SEED is not None else random.Random()\n",
    "\n",
    "    print(f\"[START] ROOT = {ROOT}\")\n",
    "    ensure_dir(ROOT)\n",
    "\n",
    "    # 1) 先创建 test/valid 目录（若不存在）\n",
    "    for base in BASES:\n",
    "        ensure_dir(ROOT / f\"{base}_test_dataset\")\n",
    "        ensure_dir(ROOT / f\"{base}_valid_dataset\")\n",
    "\n",
    "    # 2) 第一次划分：train -> test (20%)\n",
    "    print(\"\\n================ Step 1: train -> test (20%) ================\")\n",
    "    for base in BASES:\n",
    "        train_dir = ROOT / f\"{base}_train_dataset\"\n",
    "        test_dir  = ROOT / f\"{base}_test_dataset\"\n",
    "\n",
    "        if not train_dir.exists():\n",
    "            print(f\"[ERROR] 找不到 train 文件夹：{train_dir} （请确认名字是否正确）\")\n",
    "            continue\n",
    "\n",
    "        move_random_subset(train_dir, test_dir, TEST_RATIO, rng, stage_name=f\"{base} train->test\")\n",
    "\n",
    "    # 3) 第二次划分：剩余 train -> valid (20% of remaining)\n",
    "    print(\"\\n================ Step 2: remaining train -> valid (20%) ================\")\n",
    "    for base in BASES:\n",
    "        train_dir = ROOT / f\"{base}_train_dataset\"\n",
    "        valid_dir = ROOT / f\"{base}_valid_dataset\"\n",
    "\n",
    "        if not train_dir.exists():\n",
    "            print(f\"[ERROR] 找不到 train 文件夹：{train_dir}\")\n",
    "            continue\n",
    "\n",
    "        move_random_subset(train_dir, valid_dir, VALID_RATIO, rng, stage_name=f\"{base} train->valid\")\n",
    "\n",
    "    # 4) 全部 12 个文件夹重新命名\n",
    "    print(\"\\n================ Step 3: Rename all 12 folders ================\")\n",
    "    for base in BASES:\n",
    "        for split in [\"train\", \"valid\", \"test\"]:\n",
    "            folder = ROOT / f\"{base}_{split}_dataset\"\n",
    "            ensure_dir(folder)\n",
    "            prefix = f\"{base}_{split}\"\n",
    "            safe_rename_all(folder, prefix)\n",
    "\n",
    "    # 5) 最终统计\n",
    "    print(\"\\n================ Final counts (12 folders) ================\")\n",
    "    total_all = 0\n",
    "    for base in BASES:\n",
    "        train_dir = ROOT / f\"{base}_train_dataset\"\n",
    "        valid_dir = ROOT / f\"{base}_valid_dataset\"\n",
    "        test_dir  = ROOT / f\"{base}_test_dataset\"\n",
    "\n",
    "        n_train = count_folder(train_dir)\n",
    "        n_valid = count_folder(valid_dir)\n",
    "        n_test  = count_folder(test_dir)\n",
    "        total = n_train + n_valid + n_test\n",
    "        total_all += total\n",
    "\n",
    "        print(f\"{base}: train={n_train}, valid={n_valid}, test={n_test} | total={total}\")\n",
    "\n",
    "    print(\"============================================================\")\n",
    "    print(f\"[DONE] 4类共计样本数（12个文件夹总和）= {total_all}\")\n",
    "    print(\"[DONE] 划分与重命名已完成（全部为移动操作）\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "FILE = \"/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_natural_earthquake.hdf5\"\n",
    "\n",
    "def print_h5_tree(f: h5py.File):\n",
    "    datasets = []\n",
    "\n",
    "    def visitor(name, obj):\n",
    "        depth = name.count(\"/\")\n",
    "        indent = \"  \" * depth\n",
    "        if isinstance(obj, h5py.Group):\n",
    "            print(f\"{indent}+ {name or '/'} (Group)  keys={len(obj.keys())}\")\n",
    "        elif isinstance(obj, h5py.Dataset):\n",
    "            datasets.append(name)\n",
    "            print(f\"{indent}- {name} (Dataset) shape={obj.shape} dtype={obj.dtype}\")\n",
    "\n",
    "    print(\"=== HDF5 结构 ===\")\n",
    "    print(\"Root keys:\", list(f.keys()))\n",
    "    f.visititems(visitor)\n",
    "    print(f\"\\n共找到 {len(datasets)} 个 Dataset\")\n",
    "    return datasets\n",
    "\n",
    "def preview_dataset(f: h5py.File, dspath: str, n_items: int = 3, n_vals: int = 20):\n",
    "    ds = f[dspath]\n",
    "    print(\"\\n=== 预览 Dataset ===\")\n",
    "    print(\"Path :\", dspath)\n",
    "    print(\"Shape:\", ds.shape)\n",
    "    print(\"Dtype:\", ds.dtype)\n",
    "    if len(ds.attrs) > 0:\n",
    "        print(\"Attrs:\", {k: ds.attrs[k] for k in ds.attrs.keys()})\n",
    "\n",
    "    # 标量 dataset\n",
    "    if ds.shape == ():\n",
    "        v = ds[()]\n",
    "        print(\"Scalar value:\", v)\n",
    "        return\n",
    "\n",
    "    # 打印前 n_items 条（每条只展示前 n_vals 个数值）\n",
    "    n0 = ds.shape[0] if ds.ndim >= 1 else 1\n",
    "    n = min(n_items, n0)\n",
    "    for i in range(n):\n",
    "        item = np.array(ds[i]) if ds.ndim >= 1 else np.array(ds[()])\n",
    "        flat = item.ravel()\n",
    "        print(f\"\\n[{i}] item.shape={item.shape} item.dtype={item.dtype}\")\n",
    "        print(f\"    first {min(n_vals, flat.size)} values:\", flat[:n_vals])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with h5py.File(FILE, \"r\") as f:\n",
    "        dsets = print_h5_tree(f)\n",
    "\n",
    "        if not dsets:\n",
    "            raise RuntimeError(\"这个文件里没有找到 Dataset（只有 Group），请检查结构。\")\n",
    "\n",
    "        # 默认预览第一个 Dataset；你也可以把这里改成你想看的路径，比如 \"waveform\" / \"data\" 等\n",
    "        target = dsets[0]\n",
    "        preview_dataset(f, target, n_items=3, n_vals=20)\n",
    "\n",
    "        print(\"\\n提示：如果你想预览特定 Dataset，把 target 改成上面打印出来的某个路径即可。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自然地震数据集增强1:采样率查看\n",
    "import h5py, json, random\n",
    "import numpy as np\n",
    "\n",
    "HDF5_PATH = \"/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_natural_earthquake.hdf5\"\n",
    "JSON_PATH = \"/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_natural_earthquake.json\"\n",
    "\n",
    "SAMPLE_N = 2000  # 抽样看 2000 条就够判断长度分布了\n",
    "\n",
    "meta = json.load(open(JSON_PATH, \"r\"))\n",
    "keys = list(meta.keys())\n",
    "random.shuffle(keys)\n",
    "keys = keys[:min(SAMPLE_N, len(keys))]\n",
    "\n",
    "lengths = []\n",
    "with h5py.File(HDF5_PATH, \"r\") as f:\n",
    "    for k in keys:\n",
    "        if k in f:\n",
    "            lengths.append(f[k].shape[0])  # 只读shape，不读波形\n",
    "\n",
    "lengths = np.array(lengths)\n",
    "print(\"抽样条数:\", len(lengths))\n",
    "print(\"min/median/max:\", lengths.min(), int(np.median(lengths)), lengths.max())\n",
    "print(\"p5/p95:\", int(np.percentile(lengths, 5)), int(np.percentile(lengths, 95)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自然地震数据集增强：重新筛选\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 路径与参数（按需修改）\n",
    "# =========================\n",
    "HDF5_PATH = \"/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_natural_earthquake.hdf5\"\n",
    "JSON_PATH = \"/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_natural_earthquake.json\"\n",
    "\n",
    "OUT_DIR = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\"\n",
    "OUT_PT  = os.path.join(OUT_DIR, \"Natural_Earthquake_45664.pt\")\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "FS = 50\n",
    "N_SAMPLES = 3000           # 60秒 * 50Hz\n",
    "P_PRE = 500                # P前10s -> 10*50=500\n",
    "S_POST = 2000              # S后40s -> 40*50=2000\n",
    "\n",
    "LOWCUT = 1\n",
    "HIGHCUT = 20\n",
    "\n",
    "STATION_MIN = 1            # 台站数门槛=1（保留更多自然地震记录）\n",
    "TRAIN_RATIO = 0.8          # 仍保持“按事件划分train/test”的逻辑（后面合并ALL）\n",
    "\n",
    "TARGET_NUM = 45664         # 目标条数：自动补足到这个数\n",
    "\n",
    "BATCH_SIZE = 512           # 预处理分批，避免一次性中间数组过大\n",
    "INITIAL_CAND_MULTIPLIER = 1.3   # 第一轮先多抽一些候选，减少补足轮数\n",
    "MAX_PASSES = 10                 # 最多补足轮数，防止死循环\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 与你 notebook 一致的逻辑\n",
    "# =========================\n",
    "def station_shuffled(ev_id_list, key_list, i=3, percentage=0.8):\n",
    "    \"\"\"\n",
    "    - 统计每个事件(ev_id)出现次数（台站/记录数）\n",
    "    - 过滤出现次数 >= i 的事件\n",
    "    - 打乱事件列表，按 percentage 划分 train/test（按事件划分）\n",
    "    - 返回 train_keys / test_keys（按 key）\n",
    "    \"\"\"\n",
    "    # 统计每个事件的记录数\n",
    "    ev_id_int = list(map(int, ev_id_list))\n",
    "    counts = Counter(ev_id_int)\n",
    "    valid_events = [ev for ev, c in counts.items() if c >= i]\n",
    "    valid_events = sorted(set(valid_events))\n",
    "    print(f\"符合台站数门槛(i={i})的总事件数: {len(valid_events)}\")\n",
    "\n",
    "    # 事件级划分\n",
    "    random.shuffle(valid_events)\n",
    "    n_train = int(len(valid_events) * percentage)\n",
    "    train_evid = set(valid_events[:n_train])\n",
    "    test_evid  = set(valid_events[n_train:])\n",
    "\n",
    "    # 把 key 分配到 train/test（加进度条）\n",
    "    train_keys, test_keys = [], []\n",
    "    for k in tqdm(key_list, desc=\"按事件划分 train/test（遍历keys）\", unit=\"key\"):\n",
    "        ev = int(str(k).split(\"_\")[0])\n",
    "        if ev in train_evid:\n",
    "            train_keys.append(k)\n",
    "        elif ev in test_evid:\n",
    "            test_keys.append(k)\n",
    "\n",
    "    return train_keys, test_keys\n",
    "\n",
    "\n",
    "def slice_pad_one_trace(h5_ds, P, S):\n",
    "    \"\"\"\n",
    "    截取：P-500 : S+2000\n",
    "    不足 3000 用通道均值补齐，超过 3000 截断到 3000\n",
    "    输出 (3, 3000)\n",
    "    \"\"\"\n",
    "    L = h5_ds.shape[0]\n",
    "    start = P - P_PRE\n",
    "    end = S + S_POST\n",
    "\n",
    "    # 边界保护\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "    if end > L:\n",
    "        end = L\n",
    "    if end <= start:\n",
    "        return None\n",
    "\n",
    "    sliced = h5_ds[start:end, :]  # (len, 3)\n",
    "    if sliced.shape[0] == 0:\n",
    "        return None\n",
    "\n",
    "    sliced = np.asarray(sliced, dtype=np.float32)\n",
    "\n",
    "    if sliced.shape[0] < N_SAMPLES:\n",
    "        means = sliced.mean(axis=0, keepdims=True)  # (1,3)\n",
    "        padded = np.empty((N_SAMPLES, 3), dtype=np.float32)\n",
    "        padded[:sliced.shape[0], :] = sliced\n",
    "        padded[sliced.shape[0]:, :] = means\n",
    "    else:\n",
    "        padded = sliced[:N_SAMPLES, :]\n",
    "\n",
    "    return padded.T  # (3, 3000)\n",
    "\n",
    "\n",
    "def build_filter(fs, lowcut, highcut, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, [low, high], btype=\"band\")\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def preprocess_batch(x, b, a):\n",
    "    \"\"\"\n",
    "    - detrend\n",
    "    - bandpass(1-20Hz)\n",
    "    - z-score（每条/每通道）\n",
    "    x: (B, 3, 3000)\n",
    "    \"\"\"\n",
    "    x = signal.detrend(x, axis=-1, type=\"linear\")\n",
    "    x = signal.filtfilt(b, a, x, axis=-1)\n",
    "\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    std[std == 0] = 1.0\n",
    "    x = (x - mean) / std\n",
    "\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "\n",
    "def try_fill_from_keys(h5f, meta, keys_list, out_arr, write_ptr, b, a, pass_id=1):\n",
    "    \"\"\"\n",
    "    从 keys_list 中持续读取、截窗、预处理并写入 out_arr，\n",
    "    直到填满或 keys_list 耗尽。\n",
    "    返回更新后的 write_ptr、已扫描的key数 processed_count、跳过数 skipped_count。\n",
    "    \"\"\"\n",
    "    batch_buf = []\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    pbar = tqdm(keys_list, desc=f\"Pass {pass_id} 扫描并填充\", unit=\"key\")\n",
    "    last_kept = write_ptr\n",
    "\n",
    "    for k in pbar:\n",
    "        processed_count += 1\n",
    "\n",
    "        # 已经填满就停止（进度条也结束）\n",
    "        if write_ptr >= out_arr.shape[0]:\n",
    "            break\n",
    "\n",
    "        if k not in h5f:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        info = meta.get(k, None)\n",
    "        if info is None:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            P = int(info[\"Pg\"])\n",
    "            S = int(info[\"Sg\"])\n",
    "        except Exception:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        trace = slice_pad_one_trace(h5f[k], P, S)\n",
    "        if trace is None:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        batch_buf.append(trace)\n",
    "\n",
    "        if len(batch_buf) >= BATCH_SIZE:\n",
    "            x = np.stack(batch_buf, axis=0)  # (B,3,3000)\n",
    "            x = preprocess_batch(x, b, a)\n",
    "\n",
    "            n = x.shape[0]\n",
    "            remain = out_arr.shape[0] - write_ptr\n",
    "            n_write = min(n, remain)\n",
    "\n",
    "            out_arr[write_ptr:write_ptr + n_write] = x[:n_write]\n",
    "            write_ptr += n_write\n",
    "            batch_buf.clear()\n",
    "\n",
    "        # 动态显示 kept\n",
    "        if write_ptr != last_kept:\n",
    "            pbar.set_postfix({\"kept\": f\"{write_ptr}/{out_arr.shape[0]}\", \"skipped\": skipped_count})\n",
    "            last_kept = write_ptr\n",
    "\n",
    "    # flush 最后一批\n",
    "    if write_ptr < out_arr.shape[0] and batch_buf:\n",
    "        x = np.stack(batch_buf, axis=0)\n",
    "        x = preprocess_batch(x, b, a)\n",
    "\n",
    "        n = x.shape[0]\n",
    "        remain = out_arr.shape[0] - write_ptr\n",
    "        n_write = min(n, remain)\n",
    "\n",
    "        out_arr[write_ptr:write_ptr + n_write] = x[:n_write]\n",
    "        write_ptr += n_write\n",
    "        batch_buf.clear()\n",
    "\n",
    "    pbar.close()\n",
    "    return write_ptr, processed_count, skipped_count\n",
    "\n",
    "\n",
    "def main():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"Step 0) 读取 JSON ...\")\n",
    "    meta = json.load(open(JSON_PATH, \"r\"))\n",
    "    all_keys = list(meta.keys())\n",
    "    print(\"JSON总key数:\", len(all_keys))\n",
    "\n",
    "    # 1) 保留 Pg / Sg 都存在的记录（加进度条）\n",
    "    filtered_keys = []\n",
    "    ev_ids = []\n",
    "\n",
    "    for k in tqdm(all_keys, desc=\"Step 1) 筛选具备 Pg&Sg 的记录\", unit=\"key\"):\n",
    "        info = meta.get(k, {})\n",
    "        if (\"Pg\" in info) and (\"Sg\" in info):\n",
    "            filtered_keys.append(k)\n",
    "            ev_ids.append(str(k).split(\"_\")[0])\n",
    "\n",
    "    print(\"同时具备 Pg & Sg 的记录数:\", len(filtered_keys))\n",
    "\n",
    "    # 2) 台站数门槛=1（按事件分组划分 train/test）\n",
    "    print(\"Step 2) 按事件台站数门槛划分 train/test ...\")\n",
    "    train_keys, test_keys = station_shuffled(\n",
    "        ev_ids, filtered_keys, i=STATION_MIN, percentage=TRAIN_RATIO\n",
    "    )\n",
    "\n",
    "    # 3) 合并为 ALL 候选池\n",
    "    pool_keys = train_keys + test_keys\n",
    "    print(\"Step 3) 候选池大小(train+test):\", len(pool_keys))\n",
    "\n",
    "    if len(pool_keys) < TARGET_NUM:\n",
    "        raise RuntimeError(f\"候选池只有 {len(pool_keys)} 条，不足目标 {TARGET_NUM}。\")\n",
    "\n",
    "    # 4) 打乱候选池（这一步很快）\n",
    "    print(\"Step 4) 打乱候选池 ...\")\n",
    "    random.shuffle(pool_keys)\n",
    "\n",
    "    # 第一轮先取 1.3 倍候选，后续自动补足\n",
    "    first_take = min(len(pool_keys), int(TARGET_NUM * INITIAL_CAND_MULTIPLIER))\n",
    "    first_keys = pool_keys[:first_take]\n",
    "    rest_keys = pool_keys[first_take:]\n",
    "\n",
    "    print(f\"Step 5) 第一轮候选数: {len(first_keys)}，剩余候选数: {len(rest_keys)}\")\n",
    "\n",
    "    # 6) 准备滤波器\n",
    "    print(\"Step 6) 构建带通滤波器(1-20Hz) ...\")\n",
    "    b, a = build_filter(FS, LOWCUT, HIGHCUT, order=4)\n",
    "\n",
    "    # 7) 预分配输出\n",
    "    print(\"Step 7) 预分配输出数组 ...\")\n",
    "    out = np.empty((TARGET_NUM, 3, N_SAMPLES), dtype=np.float32)\n",
    "    write_ptr = 0\n",
    "\n",
    "    print(\"Step 8) 打开 HDF5 并开始填充（自动补足） ...\")\n",
    "    with h5py.File(HDF5_PATH, \"r\") as h5f:\n",
    "        # Pass 1\n",
    "        write_ptr, processed, skipped = try_fill_from_keys(\n",
    "            h5f, meta, first_keys, out, write_ptr, b, a, pass_id=1\n",
    "        )\n",
    "        print(f\"[Pass 1] scanned={processed}, skipped={skipped}, kept={write_ptr}/{TARGET_NUM}\")\n",
    "\n",
    "        # 多轮补足\n",
    "        pass_id = 1\n",
    "        while write_ptr < TARGET_NUM and rest_keys and pass_id < MAX_PASSES:\n",
    "            pass_id += 1\n",
    "            need = TARGET_NUM - write_ptr\n",
    "\n",
    "            take = min(len(rest_keys), int(need * INITIAL_CAND_MULTIPLIER))\n",
    "            take = max(take, need)  # 至少拿 need 条候选\n",
    "            cur = rest_keys[:take]\n",
    "            rest_keys = rest_keys[take:]\n",
    "\n",
    "            write_ptr, processed, skipped = try_fill_from_keys(\n",
    "                h5f, meta, cur, out, write_ptr, b, a, pass_id=pass_id\n",
    "            )\n",
    "            print(f\"[Pass {pass_id}] scanned={processed}, skipped={skipped}, kept={write_ptr}/{TARGET_NUM}\")\n",
    "\n",
    "        if write_ptr < TARGET_NUM:\n",
    "            raise RuntimeError(\n",
    "                f\"自动补足失败：最终 kept={write_ptr}/{TARGET_NUM}，剩余候选={len(rest_keys)}。\"\n",
    "            )\n",
    "\n",
    "    # 9) 保存\n",
    "    print(\"Step 9) 保存为 .pt ...\")\n",
    "    out_tensor = torch.from_numpy(out)  # (45664,3,3000)\n",
    "    torch.save(out_tensor, OUT_PT)\n",
    "    print(\"Saved:\", OUT_PT)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "PT_PATH = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement/Natural_Earthquake_45664.pt\"\n",
    "\n",
    "# 1) 加载\n",
    "x = torch.load(PT_PATH, map_location=\"cpu\")\n",
    "\n",
    "print(\"=== 基本信息 ===\")\n",
    "print(\"Type:\", type(x))\n",
    "if isinstance(x, torch.Tensor):\n",
    "    print(\"Shape:\", tuple(x.shape))   # 期望 (45664, 3, 3000)\n",
    "    print(\"Dtype:\", x.dtype)\n",
    "    print(\"Device:\", x.device)\n",
    "else:\n",
    "    # 如果你未来保存成 dict，也能看到 keys\n",
    "    print(\"Keys:\", getattr(x, \"keys\", lambda: None)())\n",
    "\n",
    "# 2) 统计信息（不会输出太多）\n",
    "xt = x.float() if isinstance(x, torch.Tensor) else None\n",
    "if xt is not None:\n",
    "    print(\"\\n=== 全局统计（抽样） ===\")\n",
    "    # 抽样 1024 条估计统计，避免全量太慢\n",
    "    n = xt.shape[0]\n",
    "    idx = torch.randperm(n)[:min(1024, n)]\n",
    "    samp = xt[idx]  # (k,3,3000)\n",
    "\n",
    "    print(\"Sample mean:\", samp.mean().item())\n",
    "    print(\"Sample std :\", samp.std().item())\n",
    "    print(\"Min/Max    :\", samp.min().item(), samp.max().item())\n",
    "    print(\"Any NaN?   :\", torch.isnan(samp).any().item())\n",
    "    print(\"Any Inf?   :\", torch.isinf(samp).any().item())\n",
    "\n",
    "# 3) 看几条样本的“前20个点”（每个通道）\n",
    "print(\"\\n=== 看前3条样本，每通道前20个点 ===\")\n",
    "for i in range(3):\n",
    "    one = xt[i]  # (3,3000)\n",
    "    print(f\"\\nSample[{i}] shape={tuple(one.shape)}\")\n",
    "    for c in range(3):\n",
    "        arr = one[c, :20].numpy()\n",
    "        print(f\"  ch{c} first20:\", np.array2string(arr, precision=4, suppress_small=True))\n",
    "\n",
    "# 4) 你也可以看某条样本的整段范围（比如 0-200点）\n",
    "i = 0\n",
    "print(\"\\n=== Sample[0] 通道0 的 0~200点（只打印简略） ===\")\n",
    "arr = xt[i, 0, :200].numpy()\n",
    "print(np.array2string(arr, precision=4, suppress_small=True))\n",
    "\n",
    "# 5) 可视化（推荐）：画一条样本 3 分量\n",
    "#    如果你是在服务器无图形界面，建议保存成 png\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 0  # 想看第几条就改这里\n",
    "one = xt[i].numpy()  # (3,3000)\n",
    "t = np.arange(one.shape[1]) / 50.0  # 50Hz -> 秒\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t, one[0], label=\"ch0\")\n",
    "plt.plot(t, one[1], label=\"ch1\")\n",
    "plt.plot(t, one[2], label=\"ch2\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude (normalized)\")\n",
    "plt.title(f\"Natural EQ sample {i} (3 components)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# 直接显示（本地/有图形界面）\n",
    "# plt.show()\n",
    "\n",
    "# 没图形界面：保存到同目录\n",
    "out_png = PT_PATH.replace(\".pt\", f\"_sample{i}.png\")\n",
    "plt.savefig(out_png, dpi=150)\n",
    "print(\"\\nSaved figure:\", out_png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ========== 路径 ==========\n",
    "NPY_DIR = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_datasets/Explosion_train_dataset\"\n",
    "PT_PATH = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement/Natural_Earthquake_45664.pt\"\n",
    "\n",
    "OUT_NPY_PATH = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement/Natural_Earthquake_45664.npy\"\n",
    "\n",
    "# ========== 工具函数 ==========\n",
    "def load_npy_any(path: str):\n",
    "    \"\"\"\n",
    "    尝试兼容：\n",
    "    - 普通 ndarray\n",
    "    - np.save 保存的 dict/tuple（需要 allow_pickle=True），通常 load 出来是 0-d object array\n",
    "    \"\"\"\n",
    "    arr = np.load(path, allow_pickle=True)\n",
    "    # 0维 object array（常见于保存 dict/tuple）\n",
    "    if isinstance(arr, np.ndarray) and arr.dtype == object and arr.shape == ():\n",
    "        return arr.item()\n",
    "    return arr\n",
    "\n",
    "def preview(obj, name=\"obj\", max_items=3):\n",
    "    print(f\"\\n===== PREVIEW: {name} =====\")\n",
    "    print(\"Type:\", type(obj))\n",
    "\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        print(\"ndarray.shape:\", obj.shape)\n",
    "        print(\"ndarray.dtype :\", obj.dtype)\n",
    "        # 只预览一点点\n",
    "        if obj.size > 0:\n",
    "            flat = obj.ravel()\n",
    "            print(\"first 20 values:\", flat[:20])\n",
    "        return\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        print(\"dict keys:\", list(obj.keys()))\n",
    "        for k in list(obj.keys())[:max_items]:\n",
    "            v = obj[k]\n",
    "            print(f\"  - key={k!r}, type={type(v)}\", end=\"\")\n",
    "            if isinstance(v, np.ndarray):\n",
    "                print(f\", shape={v.shape}, dtype={v.dtype}\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "        return\n",
    "\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        print(\"len:\", len(obj))\n",
    "        for i, v in enumerate(obj[:max_items]):\n",
    "            print(f\"  - [{i}] type={type(v)}\", end=\"\")\n",
    "            if isinstance(v, np.ndarray):\n",
    "                print(f\", shape={v.shape}, dtype={v.dtype}\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "        return\n",
    "\n",
    "    # 其它类型就直接打印 repr 的前一段\n",
    "    s = repr(obj)\n",
    "    print(\"repr(head):\", s[:300])\n",
    "\n",
    "def guess_data_key(example_dict: dict):\n",
    "    \"\"\"\n",
    "    如果示例 .npy 是 dict，猜测哪个 key 是用来存波形数据的\n",
    "    \"\"\"\n",
    "    candidates = [\"data\", \"x\", \"X\", \"waveform\", \"waveforms\", \"signals\", \"signal\"]\n",
    "    for k in candidates:\n",
    "        if k in example_dict:\n",
    "            return k\n",
    "\n",
    "    # 再退一步：找第一个 ndarray 且维度>=2 的 key\n",
    "    for k, v in example_dict.items():\n",
    "        if isinstance(v, np.ndarray) and v.ndim >= 2:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "# ========== 主逻辑 ==========\n",
    "def main():\n",
    "    # 1) 找一个示例 .npy 并打印它长什么样\n",
    "    npy_files = sorted(glob.glob(os.path.join(NPY_DIR, \"*.npy\")))\n",
    "    if not npy_files:\n",
    "        raise FileNotFoundError(f\"目录里没找到 .npy: {NPY_DIR}\")\n",
    "\n",
    "    example_path = npy_files[0]\n",
    "    print(\"Example .npy:\", example_path)\n",
    "\n",
    "    example_obj = load_npy_any(example_path)\n",
    "    preview(example_obj, \"EXAMPLE_NPY\")\n",
    "\n",
    "    # 2) 读取 .pt\n",
    "    x = torch.load(PT_PATH, map_location=\"cpu\")\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        raise TypeError(f\"你的 .pt 不是 Tensor，而是 {type(x)}，请把加载出来的结构贴我。\")\n",
    "    x_np = x.detach().cpu().numpy()\n",
    "    print(\"\\nLoaded PT tensor:\", x_np.shape, x_np.dtype)\n",
    "\n",
    "    # 3) 按示例格式保存为 .npy（自动适配：ndarray / dict / tuple）\n",
    "    os.makedirs(os.path.dirname(OUT_NPY_PATH), exist_ok=True)\n",
    "\n",
    "    if isinstance(example_obj, np.ndarray):\n",
    "        # 目标就是保存成一个 ndarray\n",
    "        out_dtype = example_obj.dtype\n",
    "        np.save(OUT_NPY_PATH, x_np.astype(out_dtype, copy=False))\n",
    "        print(\"\\nSaved as ndarray npy:\", OUT_NPY_PATH)\n",
    "\n",
    "    elif isinstance(example_obj, dict):\n",
    "        # 尽量复刻 dict 结构：把数据放进“数据 key”\n",
    "        out = dict(example_obj)  # 浅拷贝结构\n",
    "        data_key = guess_data_key(example_obj)\n",
    "        if data_key is None:\n",
    "            # 找不到就新建一个 key\n",
    "            data_key = \"data\"\n",
    "        # dtype 尽量跟示例一致\n",
    "        if data_key in example_obj and isinstance(example_obj[data_key], np.ndarray):\n",
    "            out_dtype = example_obj[data_key].dtype\n",
    "        else:\n",
    "            out_dtype = x_np.dtype\n",
    "\n",
    "        out[data_key] = x_np.astype(out_dtype, copy=False)\n",
    "        np.save(OUT_NPY_PATH, out, allow_pickle=True)\n",
    "        print(f\"\\nSaved as dict npy (data key={data_key!r}):\", OUT_NPY_PATH)\n",
    "\n",
    "    elif isinstance(example_obj, (list, tuple)):\n",
    "        # 如果示例是 tuple/list，通常第0个是数据，第1个可能是标签等\n",
    "        out_list = list(example_obj)\n",
    "        # 把第0个替换成数据（如果第0个不是 ndarray，你就把输出贴我再定）\n",
    "        out_dtype = x_np.dtype\n",
    "        if len(out_list) > 0 and isinstance(out_list[0], np.ndarray):\n",
    "            out_dtype = out_list[0].dtype\n",
    "        if len(out_list) == 0:\n",
    "            out_list = [x_np.astype(out_dtype, copy=False)]\n",
    "        else:\n",
    "            out_list[0] = x_np.astype(out_dtype, copy=False)\n",
    "\n",
    "        np.save(OUT_NPY_PATH, tuple(out_list), allow_pickle=True)\n",
    "        print(\"\\nSaved as tuple/list npy:\", OUT_NPY_PATH)\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"示例 .npy 格式太特殊：{type(example_obj)}，把 preview 输出贴我我再给你定制保存。\")\n",
    "\n",
    "    # 4) 复查一下保存结果（再打印一遍结构）\n",
    "    saved_obj = load_npy_any(OUT_NPY_PATH)\n",
    "    preview(saved_obj, \"SAVED_NPY_CHECK\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "PT_PATH = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement/Natural_Earthquake_45664.pt\"\n",
    "\n",
    "SRC_TEST_DIR  = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_datasets/Earthquake_test_dataset\"\n",
    "SRC_TRAIN_DIR = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_datasets/Earthquake_train_dataset\"\n",
    "SRC_VALID_DIR = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_datasets/Earthquake_valid_dataset\"\n",
    "\n",
    "DST_BASE = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\"\n",
    "DST_TEST_DIR  = os.path.join(DST_BASE, \"Earthquake_test_dataset_enhancement\")\n",
    "DST_TRAIN_DIR = os.path.join(DST_BASE, \"Earthquake_train_dataset_enhancement\")\n",
    "DST_VALID_DIR = os.path.join(DST_BASE, \"Earthquake_valid_dataset_enhancement\")\n",
    "\n",
    "# =========================\n",
    "# Split config\n",
    "# =========================\n",
    "SEED = 0\n",
    "N_TOTAL = 45664\n",
    "N_TEST_EXTRA = 9278  # from pt -> test enhancement\n",
    "# remaining = 36386 -> train/valid 80/20\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# Save format\n",
    "SAVE_DTYPE = np.float64  # match your sample .npy dtype\n",
    "\n",
    "# Filename prefix to avoid collisions with copied files\n",
    "PREFIX_TEST  = \"gen_test_\"\n",
    "PREFIX_TRAIN = \"gen_train_\"\n",
    "PREFIX_VALID = \"gen_valid_\"\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def ensure_dir(path: str):\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"[OK] Ensure dir: {path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] Ensure dir: {path} | {e}\")\n",
    "        return False\n",
    "\n",
    "def list_npy_files(dir_path: str):\n",
    "    return sorted(glob.glob(os.path.join(dir_path, \"*.npy\")))\n",
    "\n",
    "def copy_all_npy(src_dir: str, dst_dir: str, step_name: str):\n",
    "    print(f\"\\n=== {step_name} ===\")\n",
    "    if not os.path.isdir(src_dir):\n",
    "        print(f\"[WARN] Source dir not found, skip copy: {src_dir}\")\n",
    "        return 0\n",
    "\n",
    "    files = list_npy_files(src_dir)\n",
    "    if not files:\n",
    "        print(f\"[WARN] No .npy found, skip copy: {src_dir}\")\n",
    "        return 0\n",
    "\n",
    "    ok = 0\n",
    "    for src in tqdm(files, desc=f\"Copying from {os.path.basename(src_dir)}\", unit=\"file\"):\n",
    "        try:\n",
    "            dst = os.path.join(dst_dir, os.path.basename(src))\n",
    "            if os.path.exists(dst):\n",
    "                # Avoid overwrite\n",
    "                base, ext = os.path.splitext(os.path.basename(src))\n",
    "                dst = os.path.join(dst_dir, f\"{base}_copy{ext}\")\n",
    "            shutil.copy2(src, dst)\n",
    "            ok += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] Copy: {src} -> {dst_dir} | {e}\")\n",
    "\n",
    "    print(f\"[OK] Copied {ok}/{len(files)} files into {dst_dir}\")\n",
    "    return ok\n",
    "\n",
    "def save_tensor_samples_to_dir(x: torch.Tensor, indices: np.ndarray, dst_dir: str, prefix: str, step_name: str):\n",
    "    \"\"\"\n",
    "    Save each sample as one .npy file. Each sample is (3, 3000).\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {step_name} ===\")\n",
    "    ok = 0\n",
    "    for j, idx in enumerate(tqdm(indices, desc=step_name, unit=\"sample\")):\n",
    "        try:\n",
    "            arr = x[int(idx)].detach().cpu().numpy()  # (3,3000)\n",
    "            arr = arr.astype(SAVE_DTYPE, copy=False)\n",
    "            out_path = os.path.join(dst_dir, f\"{prefix}{j:06d}.npy\")\n",
    "            np.save(out_path, arr)\n",
    "            ok += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] Save idx={idx} to {dst_dir} | {e}\")\n",
    "\n",
    "    print(f\"[OK] Saved {ok}/{len(indices)} samples into {dst_dir}\")\n",
    "    return ok\n",
    "\n",
    "def count_npy(dir_path: str):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        return 0\n",
    "    return len(list_npy_files(dir_path))\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"Step 0) Create destination dirs\")\n",
    "    if not all([\n",
    "        ensure_dir(DST_TEST_DIR),\n",
    "        ensure_dir(DST_TRAIN_DIR),\n",
    "        ensure_dir(DST_VALID_DIR),\n",
    "    ]):\n",
    "        raise RuntimeError(\"Destination directory creation failed.\")\n",
    "\n",
    "    print(\"\\nStep 1) Load .pt tensor\")\n",
    "    try:\n",
    "        x = torch.load(PT_PATH, map_location=\"cpu\")\n",
    "        print(f\"[OK] Loaded PT: type={type(x)}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Load PT failed: {e}\")\n",
    "\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        raise TypeError(f\"PT content is not a Tensor, got: {type(x)}\")\n",
    "\n",
    "    if tuple(x.shape) != (N_TOTAL, 3, 3000):\n",
    "        raise ValueError(f\"Unexpected tensor shape: {tuple(x.shape)} (expected {(N_TOTAL,3,3000)})\")\n",
    "    print(f\"[OK] Tensor shape={tuple(x.shape)} dtype={x.dtype}\")\n",
    "\n",
    "    print(\"\\nStep 2) Split indices (no overlap)\")\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    all_idx = np.arange(N_TOTAL)\n",
    "    rng.shuffle(all_idx)\n",
    "\n",
    "    test_idx = all_idx[:N_TEST_EXTRA]\n",
    "    rest_idx = all_idx[N_TEST_EXTRA:]  # 36386\n",
    "\n",
    "    train_n = int(TRAIN_RATIO * len(rest_idx))  # floor\n",
    "    train_idx = rest_idx[:train_n]\n",
    "    valid_idx = rest_idx[train_n:]\n",
    "\n",
    "    print(f\"[INFO] test_extra={len(test_idx)}\")\n",
    "    print(f\"[INFO] rest={len(rest_idx)} -> train_extra={len(train_idx)} valid_extra={len(valid_idx)}\")\n",
    "    assert len(test_idx) + len(train_idx) + len(valid_idx) == N_TOTAL\n",
    "    assert len(set(test_idx).intersection(set(train_idx))) == 0\n",
    "    assert len(set(test_idx).intersection(set(valid_idx))) == 0\n",
    "    assert len(set(train_idx).intersection(set(valid_idx))) == 0\n",
    "    print(\"[OK] Split verified: no overlaps, total matches.\")\n",
    "\n",
    "    print(\"\\nStep 3) Copy existing Earthquake_test_dataset .npy -> enhancement/test\")\n",
    "    copied_test = copy_all_npy(SRC_TEST_DIR, DST_TEST_DIR, \"Copy existing TEST dataset\")\n",
    "\n",
    "    print(\"\\nStep 4) Save 9278 extra test samples from PT -> enhancement/test (one file per sample)\")\n",
    "    saved_test = save_tensor_samples_to_dir(\n",
    "        x, test_idx, DST_TEST_DIR, PREFIX_TEST,\n",
    "        step_name=f\"Save PT->TEST {len(test_idx)} samples\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nStep 5) Copy existing Earthquake_train_dataset .npy -> enhancement/train\")\n",
    "    copied_train = copy_all_npy(SRC_TRAIN_DIR, DST_TRAIN_DIR, \"Copy existing TRAIN dataset\")\n",
    "\n",
    "    print(\"\\nStep 6) Save extra train samples from PT -> enhancement/train (one file per sample)\")\n",
    "    saved_train = save_tensor_samples_to_dir(\n",
    "        x, train_idx, DST_TRAIN_DIR, PREFIX_TRAIN,\n",
    "        step_name=f\"Save PT->TRAIN {len(train_idx)} samples\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nStep 7) Copy existing Earthquake_valid_dataset .npy -> enhancement/valid\")\n",
    "    copied_valid = copy_all_npy(SRC_VALID_DIR, DST_VALID_DIR, \"Copy existing VALID dataset\")\n",
    "\n",
    "    print(\"\\nStep 8) Save extra valid samples from PT -> enhancement/valid (one file per sample)\")\n",
    "    saved_valid = save_tensor_samples_to_dir(\n",
    "        x, valid_idx, DST_VALID_DIR, PREFIX_VALID,\n",
    "        step_name=f\"Save PT->VALID {len(valid_idx)} samples\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nStep 9) Final counts (number of .npy files)\")\n",
    "    n_test = count_npy(DST_TEST_DIR)\n",
    "    n_train = count_npy(DST_TRAIN_DIR)\n",
    "    n_valid = count_npy(DST_VALID_DIR)\n",
    "\n",
    "    print(f\"[RESULT] {DST_TEST_DIR}  files: {n_test} (copied={copied_test}, saved={saved_test})\")\n",
    "    print(f\"[RESULT] {DST_TRAIN_DIR} files: {n_train} (copied={copied_train}, saved={saved_train})\")\n",
    "    print(f\"[RESULT] {DST_VALID_DIR} files: {n_valid} (copied={copied_valid}, saved={saved_valid})\")\n",
    "\n",
    "    # Your expected test total:\n",
    "    print(\"\\n[CHECK] Expected test total = existing_test + 9278\")\n",
    "    print(f\"        existing_test_copied={copied_test}, +9278 => {copied_test + N_TEST_EXTRA}\")\n",
    "    print(f\"        actual_test_files={n_test}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 需要重命名的三个目录\n",
    "BASE = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\"\n",
    "DIRS = [\n",
    "    os.path.join(BASE, \"Earthquake_train_dataset_enhancement\"),\n",
    "    os.path.join(BASE, \"Earthquake_valid_dataset_enhancement\"),\n",
    "    os.path.join(BASE, \"Earthquake_test_dataset_enhancement\"),\n",
    "]\n",
    "\n",
    "# 文件名格式：<foldername>_00001.npy\n",
    "PAD = 5  # 00001 -> 5位补零\n",
    "DRY_RUN = False  # True=只打印不改名；False=真正执行\n",
    "\n",
    "def list_npy_files(dir_path: str):\n",
    "    return sorted(glob.glob(os.path.join(dir_path, \"*.npy\")))\n",
    "\n",
    "def rename_in_folder(dir_path: str):\n",
    "    folder = os.path.basename(dir_path.rstrip(\"/\"))\n",
    "    files = list_npy_files(dir_path)\n",
    "    if not files:\n",
    "        print(f\"[WARN] no .npy in {dir_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=== Renaming in {dir_path} (n={len(files)}) ===\")\n",
    "\n",
    "    # 先做两步改名，避免与目标文件名冲突：\n",
    "    # Step A: 先改成临时名\n",
    "    tmp_paths = []\n",
    "    for i, old in enumerate(tqdm(files, desc=f\"{folder}: temp rename\", unit=\"file\")):\n",
    "        tmp = os.path.join(dir_path, f\".__tmp__{i:08d}.npy\")\n",
    "        if DRY_RUN:\n",
    "            print(f\"[DRY] {old} -> {tmp}\")\n",
    "        else:\n",
    "            os.rename(old, tmp)\n",
    "        tmp_paths.append(tmp)\n",
    "\n",
    "    # Step B: 再从临时名改成最终名（从00001开始）\n",
    "    for i, tmp in enumerate(tqdm(tmp_paths, desc=f\"{folder}: final rename\", unit=\"file\"), start=1):\n",
    "        new = os.path.join(dir_path, f\"{folder}_{i:0{PAD}d}.npy\")\n",
    "        if DRY_RUN:\n",
    "            print(f\"[DRY] {tmp} -> {new}\")\n",
    "        else:\n",
    "            os.rename(tmp, new)\n",
    "\n",
    "    print(f\"[OK] Renamed {len(files)} files in {dir_path} -> {folder}_00001.npy ...\")\n",
    "\n",
    "def main():\n",
    "    for d in DIRS:\n",
    "        if not os.path.isdir(d):\n",
    "            print(f\"[FAIL] dir not found: {d}\")\n",
    "            continue\n",
    "        rename_in_folder(d)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\"\n",
    "FOLDERS = [\n",
    "    os.path.join(BASE, \"Earthquake_train_dataset_enhancement\"),\n",
    "    os.path.join(BASE, \"Earthquake_valid_dataset_enhancement\"),\n",
    "    os.path.join(BASE, \"Earthquake_test_dataset_enhancement\"),\n",
    "]\n",
    "\n",
    "# 判定“全0”的标准：整个数组没有任何非零元素\n",
    "# （如果你想容忍极小数值，把判断改成 np.all(np.abs(arr) < eps)）\n",
    "EPS = 0.0\n",
    "\n",
    "def list_npy(dir_path: str):\n",
    "    return sorted(glob.glob(os.path.join(dir_path, \"*.npy\")))\n",
    "\n",
    "def is_all_zero(arr: np.ndarray, eps: float = 0.0) -> bool:\n",
    "    if eps == 0.0:\n",
    "        return np.count_nonzero(arr) == 0\n",
    "    return np.all(np.abs(arr) <= eps)\n",
    "\n",
    "def scan_folder(folder: str, eps: float = 0.0, show_examples: int = 5):\n",
    "    files = list_npy(folder)\n",
    "    total = len(files)\n",
    "    zero_files = []\n",
    "\n",
    "    for f in tqdm(files, desc=f\"Scanning {os.path.basename(folder)}\", unit=\"file\"):\n",
    "        try:\n",
    "            # mmap_mode 不把全文件读进内存\n",
    "            arr = np.load(f, mmap_mode=\"r\", allow_pickle=False)\n",
    "            if is_all_zero(arr, eps=eps):\n",
    "                zero_files.append(f)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[FAIL] Load {f}: {e}\")\n",
    "\n",
    "    print(\"\\n========================================\")\n",
    "    print(\"Folder:\", folder)\n",
    "    print(\"Total files:\", total)\n",
    "    print(\"All-zero files:\", len(zero_files))\n",
    "    if zero_files:\n",
    "        print(\"Examples:\")\n",
    "        for p in zero_files[:show_examples]:\n",
    "            print(\"  -\", p)\n",
    "    return total, len(zero_files)\n",
    "\n",
    "def main():\n",
    "    print(f\"All-zero criterion: eps={EPS} (eps=0 means exactly all zeros)\\n\")\n",
    "\n",
    "    summary = []\n",
    "    for folder in FOLDERS:\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"[FAIL] Folder not found: {folder}\")\n",
    "            continue\n",
    "        summary.append(scan_folder(folder, eps=EPS, show_examples=5))\n",
    "\n",
    "    print(\"\\n========== SUMMARY ==========\")\n",
    "    for folder, (total, zeros) in zip([os.path.basename(f) for f in FOLDERS], summary):\n",
    "        print(f\"{folder}: total={total}, all_zero={zeros}\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换三个 enhancement 文件夹中的“全0 .npy”为新提取的“非0”自然地震样本\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 原始自然地震数据（用于重新提取）\n",
    "# =========================\n",
    "HDF5_PATH = \"/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_natural_earthquake.hdf5\"\n",
    "JSON_PATH = \"/home/zypei/DiTing2.0_dataset/CENC_DiTingv2/CENC_DiTingv2_natural_earthquake.json\"\n",
    "\n",
    "# =========================\n",
    "# 三个需要替换全0文件的目录\n",
    "# =========================\n",
    "BASE_ENH = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\"\n",
    "TRAIN_DIR = os.path.join(BASE_ENH, \"Earthquake_train_dataset_enhancement\")\n",
    "VALID_DIR = os.path.join(BASE_ENH, \"Earthquake_valid_dataset_enhancement\")\n",
    "TEST_DIR  = os.path.join(BASE_ENH, \"Earthquake_test_dataset_enhancement\")\n",
    "\n",
    "FOLDERS = [TRAIN_DIR, VALID_DIR, TEST_DIR]\n",
    "\n",
    "# =========================\n",
    "# 处理参数（保持你之前逻辑）\n",
    "# =========================\n",
    "SEED = 0\n",
    "\n",
    "FS = 50\n",
    "N_SAMPLES = 3000\n",
    "P_PRE = 500\n",
    "S_POST = 2000\n",
    "\n",
    "LOWCUT = 1\n",
    "HIGHCUT = 20\n",
    "\n",
    "STATION_MIN = 1\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "SAVE_DTYPE = np.float64  # 你的 .npy 示例是 float64\n",
    "\n",
    "# 防止循环太久：最多扫描多少个候选key（通常用不到这么多）\n",
    "MAX_SCAN_KEYS = 2_000_000\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 基础函数\n",
    "# =========================\n",
    "def list_npy_files(folder: str):\n",
    "    return sorted(glob.glob(os.path.join(folder, \"*.npy\")))\n",
    "\n",
    "def is_all_zero_npy(path: str) -> bool:\n",
    "    try:\n",
    "        arr = np.load(path, mmap_mode=\"r\", allow_pickle=False)\n",
    "        return np.count_nonzero(arr) == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def scan_all_zero_files(folder: str):\n",
    "    files = list_npy_files(folder)\n",
    "    zero_files = []\n",
    "    for f in tqdm(files, desc=f\"扫描全0文件: {os.path.basename(folder)}\", unit=\"file\"):\n",
    "        try:\n",
    "            arr = np.load(f, mmap_mode=\"r\", allow_pickle=False)\n",
    "            if np.count_nonzero(arr) == 0:\n",
    "                zero_files.append(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] 读取失败: {f} | {e}\")\n",
    "    return zero_files\n",
    "\n",
    "def station_shuffled(ev_id_list, key_list, i=3, percentage=0.8):\n",
    "    \"\"\"\n",
    "    - 统计每个事件(ev_id)出现次数（台站/记录数）\n",
    "    - 过滤出现次数 >= i 的事件\n",
    "    - 打乱事件列表，按 percentage 划分 train/test（按事件划分）\n",
    "    - 返回 train_keys / test_keys（按 key）\n",
    "    \"\"\"\n",
    "    ev_id_int = list(map(int, ev_id_list))\n",
    "    counts = Counter(ev_id_int)\n",
    "    valid_events = [ev for ev, c in counts.items() if c >= i]\n",
    "    valid_events = sorted(set(valid_events))\n",
    "    print(f\"[INFO] 符合台站数门槛(i={i})的总事件数: {len(valid_events)}\")\n",
    "\n",
    "    random.shuffle(valid_events)\n",
    "    n_train = int(len(valid_events) * percentage)\n",
    "    train_evid = set(valid_events[:n_train])\n",
    "    test_evid  = set(valid_events[n_train:])\n",
    "\n",
    "    train_keys, test_keys = [], []\n",
    "    for k in tqdm(key_list, desc=\"按事件划分 train/test（遍历keys）\", unit=\"key\"):\n",
    "        ev = int(str(k).split(\"_\")[0])\n",
    "        if ev in train_evid:\n",
    "            train_keys.append(k)\n",
    "        elif ev in test_evid:\n",
    "            test_keys.append(k)\n",
    "    return train_keys, test_keys\n",
    "\n",
    "def slice_pad_one_trace(h5_ds, P, S):\n",
    "    \"\"\"\n",
    "    截取：P-500 : S+2000\n",
    "    不足 3000 用通道均值补齐，超过 3000 截断到 3000\n",
    "    输出 (3, 3000)\n",
    "    \"\"\"\n",
    "    L = h5_ds.shape[0]\n",
    "    start = P - P_PRE\n",
    "    end = S + S_POST\n",
    "\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "    if end > L:\n",
    "        end = L\n",
    "    if end <= start:\n",
    "        return None\n",
    "\n",
    "    sliced = h5_ds[start:end, :]  # (len,3)\n",
    "    if sliced.shape[0] == 0:\n",
    "        return None\n",
    "\n",
    "    sliced = np.asarray(sliced, dtype=np.float32)\n",
    "\n",
    "    if sliced.shape[0] < N_SAMPLES:\n",
    "        means = sliced.mean(axis=0, keepdims=True)  # (1,3)\n",
    "        padded = np.empty((N_SAMPLES, 3), dtype=np.float32)\n",
    "        padded[:sliced.shape[0], :] = sliced\n",
    "        padded[sliced.shape[0]:, :] = means\n",
    "    else:\n",
    "        padded = sliced[:N_SAMPLES, :]\n",
    "\n",
    "    return padded.T  # (3,3000)\n",
    "\n",
    "def build_filter(fs, lowcut, highcut, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = signal.butter(order, [low, high], btype=\"band\")\n",
    "    return b, a\n",
    "\n",
    "def preprocess_one(x_3x3000, b, a):\n",
    "    \"\"\"\n",
    "    单条样本版本：\n",
    "    - detrend\n",
    "    - bandpass(1-20Hz)\n",
    "    - z-score（每通道）\n",
    "    输入输出: (3,3000) float32\n",
    "    \"\"\"\n",
    "    x = x_3x3000.astype(np.float32, copy=False)\n",
    "\n",
    "    x = signal.detrend(x, axis=-1, type=\"linear\")\n",
    "    x = signal.filtfilt(b, a, x, axis=-1)\n",
    "\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std = x.std(axis=-1, keepdims=True)\n",
    "    std[std == 0] = 1.0\n",
    "    x = (x - mean) / std\n",
    "\n",
    "    return x.astype(np.float32, copy=False)\n",
    "\n",
    "def is_all_zero_arr(arr: np.ndarray) -> bool:\n",
    "    return np.count_nonzero(arr) == 0\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 主流程\n",
    "# =========================\n",
    "def main():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print(\"Step 1) 扫描三个文件夹中的全0 .npy 文件 ...\")\n",
    "    zero_map = {}\n",
    "    all_zero_files = []\n",
    "    for folder in FOLDERS:\n",
    "        if not os.path.isdir(folder):\n",
    "            raise FileNotFoundError(f\"目录不存在: {folder}\")\n",
    "        zeros = scan_all_zero_files(folder)\n",
    "        zero_map[folder] = zeros\n",
    "        all_zero_files.extend(zeros)\n",
    "        print(f\"[OK] {os.path.basename(folder)} 全0文件数: {len(zeros)}\")\n",
    "\n",
    "    need = len(all_zero_files)\n",
    "    print(f\"\\n[INFO] 总共需要替换的全0文件数 = {need}\")\n",
    "    if need == 0:\n",
    "        print(\"[DONE] 没有全0文件，不需要替换。\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nStep 2) 读取 JSON 并构造候选 key 池（Pg&Sg + station_min=1） ...\")\n",
    "    meta = json.load(open(JSON_PATH, \"r\"))\n",
    "    all_keys = list(meta.keys())\n",
    "    print(\"[INFO] JSON总key数:\", len(all_keys))\n",
    "\n",
    "    filtered_keys = []\n",
    "    ev_ids = []\n",
    "    for k in tqdm(all_keys, desc=\"筛选具备 Pg&Sg 的记录\", unit=\"key\"):\n",
    "        info = meta.get(k, {})\n",
    "        if (\"Pg\" in info) and (\"Sg\" in info):\n",
    "            filtered_keys.append(k)\n",
    "            ev_ids.append(str(k).split(\"_\")[0])\n",
    "    print(\"[INFO] 同时具备 Pg & Sg 的记录数:\", len(filtered_keys))\n",
    "\n",
    "    train_keys, test_keys = station_shuffled(ev_ids, filtered_keys, i=STATION_MIN, percentage=TRAIN_RATIO)\n",
    "    pool_keys = train_keys + test_keys\n",
    "    print(\"[INFO] 候选池大小(train+test):\", len(pool_keys))\n",
    "    if len(pool_keys) == 0:\n",
    "        raise RuntimeError(\"候选池为空，无法继续。\")\n",
    "\n",
    "    random.shuffle(pool_keys)\n",
    "\n",
    "    print(\"\\nStep 3) 从原始 HDF5 中继续提取“非全0”的新样本，用来替换 ...\")\n",
    "    b, a = build_filter(FS, LOWCUT, HIGHCUT, order=4)\n",
    "\n",
    "    replacements = []\n",
    "    used_keys = set()\n",
    "\n",
    "    with h5py.File(HDF5_PATH, \"r\") as h5f:\n",
    "        scan_limit = min(len(pool_keys), MAX_SCAN_KEYS)\n",
    "        pbar = tqdm(pool_keys[:scan_limit], desc=\"生成替换样本(非0)\", unit=\"key\")\n",
    "        for k in pbar:\n",
    "            if len(replacements) >= need:\n",
    "                break\n",
    "            if k in used_keys:\n",
    "                continue\n",
    "            if k not in h5f:\n",
    "                continue\n",
    "\n",
    "            info = meta.get(k, None)\n",
    "            if info is None:\n",
    "                continue\n",
    "            try:\n",
    "                P = int(info[\"Pg\"])\n",
    "                S = int(info[\"Sg\"])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            trace = slice_pad_one_trace(h5f[k], P, S)\n",
    "            if trace is None:\n",
    "                continue\n",
    "\n",
    "            x = preprocess_one(trace, b, a)  # (3,3000) float32\n",
    "\n",
    "            # 关键：跳过全0 / NaN / Inf\n",
    "            if is_all_zero_arr(x):\n",
    "                continue\n",
    "            if np.isnan(x).any() or np.isinf(x).any():\n",
    "                continue\n",
    "\n",
    "            replacements.append(x.astype(SAVE_DTYPE, copy=False))  # 保存为 float64\n",
    "            used_keys.add(k)\n",
    "\n",
    "            pbar.set_postfix({\"got\": f\"{len(replacements)}/{need}\"})\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    if len(replacements) < need:\n",
    "        raise RuntimeError(f\"替换样本不足：只生成了 {len(replacements)} 条，但需要 {need} 条。\")\n",
    "\n",
    "    print(f\"[OK] 成功生成替换样本: {len(replacements)} 条（全部非全0）\")\n",
    "\n",
    "    print(\"\\nStep 4) 覆盖写回：用新样本替换全0 .npy（文件名保持不变） ...\")\n",
    "    # 为了可重复，把全0文件路径排序后按顺序替换\n",
    "    all_zero_files_sorted = sorted(all_zero_files)\n",
    "\n",
    "    ok, fail = 0, 0\n",
    "    for i, fpath in enumerate(tqdm(all_zero_files_sorted, desc=\"写回替换文件\", unit=\"file\")):\n",
    "        try:\n",
    "            # 再确认一次当前文件确实全0（防止重复运行时误覆盖）\n",
    "            arr_old = np.load(fpath, mmap_mode=\"r\", allow_pickle=False)\n",
    "            if np.count_nonzero(arr_old) != 0:\n",
    "                # 已经不是全0了，跳过（不算失败）\n",
    "                continue\n",
    "\n",
    "            np.save(fpath, replacements[i])  # 覆盖写回\n",
    "            ok += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] 写回失败: {fpath} | {e}\")\n",
    "            fail += 1\n",
    "\n",
    "    print(f\"[OK] 写回完成：success={ok}, fail={fail}\")\n",
    "\n",
    "    print(\"\\nStep 5) 重新统计三个文件夹的全0文件数量（验证替换效果） ...\")\n",
    "    for folder in FOLDERS:\n",
    "        zeros = scan_all_zero_files(folder)\n",
    "        print(f\"[RESULT] {os.path.basename(folder)} 现在全0文件数: {len(zeros)}\")\n",
    "\n",
    "    print(\"\\nDONE.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# 你要看的三个目录\n",
    "# =========================\n",
    "BASE = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\"\n",
    "FOLDERS = [\n",
    "    os.path.join(BASE, \"Earthquake_train_dataset_enhancement\"),\n",
    "    os.path.join(BASE, \"Earthquake_valid_dataset_enhancement\"),\n",
    "    os.path.join(BASE, \"Earthquake_test_dataset_enhancement\"),\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# 可调参数（你改这里就行）\n",
    "# =========================\n",
    "SEED = 0\n",
    "\n",
    "SAMPLES_PER_FOLDER = 5   # 每个文件夹要打印多少个 .npy\n",
    "RANDOM_PICK = True       # True=随机抽样；False=按文件名顺序取前几个\n",
    "\n",
    "START_IDX = 0            # 从第几个采样点开始展示\n",
    "SHOW_POINTS = 100         # 展示多少个采样点（每通道）\n",
    "CHANNELS = [0, 1, 2]     # 想看哪些通道：0/1/2\n",
    "\n",
    "PRINT_FULL_STATS = True  # 是否打印 min/max/std/非零数 等统计\n",
    "ONLY_SHOW_ALL_ZERO = False  # True=只打印全0文件；False=都可能打印\n",
    "\n",
    "# 全0判定阈值：0=严格全0；例如 1e-12 可视为“几乎全0”\n",
    "ZERO_EPS = 0.0\n",
    "\n",
    "# =========================\n",
    "# 工具函数\n",
    "# =========================\n",
    "def list_npy(folder: str):\n",
    "    return sorted(glob.glob(os.path.join(folder, \"*.npy\")))\n",
    "\n",
    "def is_all_zero(arr: np.ndarray, eps: float = 0.0) -> bool:\n",
    "    if eps == 0.0:\n",
    "        return np.count_nonzero(arr) == 0\n",
    "    return np.all(np.abs(arr) <= eps)\n",
    "\n",
    "def preview_file(path: str):\n",
    "    arr = np.load(path, allow_pickle=False)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"File :\", path)\n",
    "    print(\"Type :\", type(arr))\n",
    "    print(\"Shape:\", arr.shape)\n",
    "    print(\"Dtype:\", arr.dtype)\n",
    "\n",
    "    if PRINT_FULL_STATS:\n",
    "        nonzero = int(np.count_nonzero(arr))\n",
    "        print(\"Nonzero:\", nonzero, \"/\", arr.size)\n",
    "        print(\"Min/Max:\", float(arr.min()), float(arr.max()))\n",
    "        # 若是 (3,3000)，打印每通道 std\n",
    "        if arr.ndim == 2 and arr.shape[0] == 3:\n",
    "            print(\"Std(ch0,ch1,ch2):\", arr.std(axis=1))\n",
    "        else:\n",
    "            print(\"Std:\", float(arr.std()))\n",
    "\n",
    "    # 打印片段\n",
    "    if arr.ndim == 2 and arr.shape[0] >= 3:\n",
    "        end = min(arr.shape[1], START_IDX + SHOW_POINTS)\n",
    "        for c in CHANNELS:\n",
    "            if c >= arr.shape[0]:\n",
    "                continue\n",
    "            seg = arr[c, START_IDX:end]\n",
    "            print(f\"\\nChannel {c} [{START_IDX}:{end}] (len={len(seg)}):\")\n",
    "            print(np.array2string(seg, precision=6, suppress_small=False))\n",
    "    else:\n",
    "        # 非标准形状就直接扁平打印\n",
    "        flat = arr.ravel()\n",
    "        end = min(flat.size, START_IDX + SHOW_POINTS)\n",
    "        seg = flat[START_IDX:end]\n",
    "        print(f\"\\nFlat [{START_IDX}:{end}] (len={len(seg)}):\")\n",
    "        print(np.array2string(seg, precision=6, suppress_small=False))\n",
    "\n",
    "def main():\n",
    "    random.seed(SEED)\n",
    "\n",
    "    for folder in FOLDERS:\n",
    "        print(\"\\n\" + \"#\" * 90)\n",
    "        print(\"Folder:\", folder)\n",
    "        if not os.path.isdir(folder):\n",
    "            print(\"[FAIL] Folder not found.\")\n",
    "            continue\n",
    "\n",
    "        files = list_npy(folder)\n",
    "        print(\"Total .npy:\", len(files))\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        # 如果只看全0文件，先筛一遍（会快一点：mmap_mode）\n",
    "        if ONLY_SHOW_ALL_ZERO:\n",
    "            zeros = []\n",
    "            for f in tqdm(files, desc=f\"筛选全0: {os.path.basename(folder)}\", unit=\"file\"):\n",
    "                try:\n",
    "                    arr = np.load(f, mmap_mode=\"r\", allow_pickle=False)\n",
    "                    if is_all_zero(arr, ZERO_EPS):\n",
    "                        zeros.append(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"[FAIL] load: {f} | {e}\")\n",
    "            files_to_pick = zeros\n",
    "            print(f\"All-zero files (eps={ZERO_EPS}): {len(files_to_pick)}\")\n",
    "            if not files_to_pick:\n",
    "                continue\n",
    "        else:\n",
    "            files_to_pick = files\n",
    "\n",
    "        k = min(SAMPLES_PER_FOLDER, len(files_to_pick))\n",
    "        if k <= 0:\n",
    "            continue\n",
    "\n",
    "        if RANDOM_PICK:\n",
    "            picked = random.sample(files_to_pick, k)\n",
    "        else:\n",
    "            picked = files_to_pick[:k]\n",
    "\n",
    "        for f in picked:\n",
    "            try:\n",
    "                preview_file(f)\n",
    "            except Exception as e:\n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"File :\", f)\n",
    "                print(\"[FAIL] Could not load/preview:\", e)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一维波形转.pt文件\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# 配置\n",
    "# =========================\n",
    "BASE_DIR = \"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\"\n",
    "OUT_DIR = BASE_DIR\n",
    "\n",
    "# 统一输出形状（模型常用）：(3, 3000)\n",
    "TARGET_SHAPE = (3, 3000)\n",
    "\n",
    "# 输出 dtype：建议 float32（训练更省显存/内存）\n",
    "SAVE_FLOAT32 = True\n",
    "\n",
    "# 是否保存文件名列表（文件很多会占空间）\n",
    "SAVE_FILENAMES = False\n",
    "\n",
    "# 是否覆盖已有 .pt\n",
    "OVERWRITE_PT = True\n",
    "\n",
    "# ✅ 固定 label2id（按你的要求）\n",
    "LABEL2ID_FIXED = {\n",
    "    \"Earthquake\": 0,\n",
    "    \"Explosion\": 1,\n",
    "    \"Collapse\": 2,\n",
    "    \"Noise\": 3,\n",
    "}\n",
    "ID2LABEL_FIXED = {v: k for k, v in LABEL2ID_FIXED.items()}\n",
    "\n",
    "def infer_label_from_folder(folder_name: str) -> str:\n",
    "    name = folder_name.lower()\n",
    "    if \"earthquake\" in name:\n",
    "        return \"Earthquake\"\n",
    "    if \"explosion\" in name:\n",
    "        return \"Explosion\"\n",
    "    if \"collapse\" in name:\n",
    "        return \"Collapse\"\n",
    "    if \"noise\" in name:\n",
    "        return \"Noise\"\n",
    "    if \"subsidence\" in name or \"sink\" in name:\n",
    "        return \"Subsidence\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def list_subfolders(base_dir: str):\n",
    "    subs = []\n",
    "    for p in sorted(os.listdir(base_dir)):\n",
    "        full = os.path.join(base_dir, p)\n",
    "        if os.path.isdir(full):\n",
    "            subs.append(full)\n",
    "    return subs\n",
    "\n",
    "\n",
    "def normalize_shape(arr: np.ndarray):\n",
    "    \"\"\"\n",
    "    把 (3,3000) 或 (3000,3) 统一转为 (3,3000)\n",
    "    返回: (arr_norm, ok)\n",
    "    \"\"\"\n",
    "    if arr.shape == TARGET_SHAPE:\n",
    "        return arr, True\n",
    "    if arr.shape == (TARGET_SHAPE[1], TARGET_SHAPE[0]):  # (3000,3)\n",
    "        return arr.T, True\n",
    "    return arr, False\n",
    "\n",
    "\n",
    "def load_folder_npy_to_tensor(folder_path: str):\n",
    "    npy_files = sorted(glob.glob(os.path.join(folder_path, \"*.npy\")))\n",
    "    if not npy_files:\n",
    "        return None, None, 0, 0, None\n",
    "\n",
    "    first = np.load(npy_files[0], allow_pickle=False)\n",
    "    first_norm, ok = normalize_shape(first)\n",
    "    if not ok:\n",
    "        print(f\"[FAIL] {folder_path}: first file shape={first.shape} not supported.\")\n",
    "        print(f\"       Supported shapes: {TARGET_SHAPE} or {(TARGET_SHAPE[1], TARGET_SHAPE[0])}\")\n",
    "        return None, None, 0, len(npy_files), first.shape\n",
    "\n",
    "    n = len(npy_files)\n",
    "    data = np.empty((n, TARGET_SHAPE[0], TARGET_SHAPE[1]), dtype=np.float32 if SAVE_FLOAT32 else first_norm.dtype)\n",
    "\n",
    "    kept_files = []\n",
    "    write_ptr = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for f in tqdm(npy_files, desc=f\"Loading {os.path.basename(folder_path)}\", unit=\"file\"):\n",
    "        try:\n",
    "            arr = np.load(f, allow_pickle=False)\n",
    "            arr, ok2 = normalize_shape(arr)\n",
    "            if not ok2:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            if SAVE_FLOAT32:\n",
    "                arr = arr.astype(np.float32, copy=False)\n",
    "\n",
    "            data[write_ptr] = arr\n",
    "            kept_files.append(os.path.basename(f))\n",
    "            write_ptr += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            print(f\"[FAIL] load {f}: {e}\")\n",
    "\n",
    "    data = data[:write_ptr]\n",
    "    x = torch.from_numpy(data)  # (N,3,3000)\n",
    "    return x, kept_files, write_ptr, skipped, first.shape\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.isdir(BASE_DIR):\n",
    "        raise FileNotFoundError(f\"BASE_DIR not found: {BASE_DIR}\")\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    subfolders = list_subfolders(BASE_DIR)\n",
    "    print(f\"[INFO] Found {len(subfolders)} subfolders in {BASE_DIR}\")\n",
    "\n",
    "    pt_written = 0\n",
    "    pt_skipped = 0\n",
    "\n",
    "    for folder in tqdm(subfolders, desc=\"Processing subfolders\", unit=\"folder\"):\n",
    "        folder_name = os.path.basename(folder.rstrip(\"/\"))\n",
    "\n",
    "        # 没有 npy 就跳过\n",
    "        npy_files = glob.glob(os.path.join(folder, \"*.npy\"))\n",
    "        if not npy_files:\n",
    "            print(f\"[SKIP] {folder_name}: no .npy files\")\n",
    "            continue\n",
    "\n",
    "        label = infer_label_from_folder(folder_name)\n",
    "\n",
    "        # 只处理你规定的4类；其它类直接跳过（避免 Unknown/Subsidence 干扰 label2id）\n",
    "        if label not in LABEL2ID_FIXED:\n",
    "            print(f\"[SKIP] {folder_name}: label={label} not in {list(LABEL2ID_FIXED.keys())}\")\n",
    "            continue\n",
    "\n",
    "        label_id = LABEL2ID_FIXED[label]\n",
    "\n",
    "        out_path = os.path.join(OUT_DIR, f\"{folder_name}.pt\")\n",
    "        if (not OVERWRITE_PT) and os.path.exists(out_path):\n",
    "            print(f\"[SKIP] {folder_name}: pt exists and OVERWRITE_PT=False -> {out_path}\")\n",
    "            pt_skipped += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Folder: {folder_name} | label={label} (id={label_id}) ===\")\n",
    "\n",
    "        x, kept_files, kept_n, skipped_n, first_shape = load_folder_npy_to_tensor(folder)\n",
    "\n",
    "        if x is None or kept_n == 0:\n",
    "            print(f\"[FAIL] {folder_name}: loaded=0, skipped={skipped_n}, first_shape={first_shape}\")\n",
    "            continue\n",
    "\n",
    "        y = torch.full((kept_n,), int(label_id), dtype=torch.long)\n",
    "\n",
    "        payload = {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"label\": label,\n",
    "            \"label_id\": int(label_id),\n",
    "            \"folder\": folder_name,\n",
    "            \"target_shape\": TARGET_SHAPE,\n",
    "            \"first_file_shape\": tuple(first_shape) if first_shape is not None else None,\n",
    "            \"dtype\": \"float32\" if SAVE_FLOAT32 else str(x.dtype),\n",
    "        }\n",
    "        if SAVE_FILENAMES:\n",
    "            payload[\"files\"] = kept_files\n",
    "\n",
    "        try:\n",
    "            torch.save(payload, out_path)\n",
    "            pt_written += 1\n",
    "            print(f\"[OK] Saved: {out_path}\")\n",
    "            print(f\"     kept={kept_n}, skipped={skipped_n}, x.shape={tuple(x.shape)}, x.dtype={x.dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] Save pt: {out_path} | {e}\")\n",
    "\n",
    "    # 保存固定映射 label2id / id2label\n",
    "    map_path = os.path.join(OUT_DIR, \"label2id.json\")\n",
    "    inv_path = os.path.join(OUT_DIR, \"id2label.json\")\n",
    "    try:\n",
    "        with open(map_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(LABEL2ID_FIXED, f, ensure_ascii=False, indent=2)\n",
    "        with open(inv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(ID2LABEL_FIXED, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"\\n[OK] Saved fixed label maps:\")\n",
    "        print(f\" - {map_path}\")\n",
    "        print(f\" - {inv_path}\")\n",
    "        print(\"[INFO] label2id =\", LABEL2ID_FIXED)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FAIL] Save label maps | {e}\")\n",
    "\n",
    "    print(f\"\\n[DONE] Total .pt written: {pt_written}, skipped existing: {pt_skipped}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.auto import tqdm  # ✅ 自动适配 notebook/终端\n",
    "import librosa\n",
    "\n",
    "# =========================\n",
    "# 路径\n",
    "# =========================\n",
    "IN_ROOT = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement\")\n",
    "OUT_ROOT = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement_MFCC\")\n",
    "\n",
    "# =========================\n",
    "# MFCC 参数（按你 notebook）\n",
    "# =========================\n",
    "SR = 50\n",
    "N_MFCC = 13\n",
    "N_FFT = 142\n",
    "HOP = 42\n",
    "WIN_LENGTH = N_FFT\n",
    "\n",
    "TARGET_FEAT = 40\n",
    "TARGET_T = 72\n",
    "\n",
    "# 是否跳过已存在输出\n",
    "SKIP_IF_EXISTS = True\n",
    "\n",
    "# 全 0 波形是否直接输出全 0 MFCC（避免 -inf）\n",
    "ZERO_WAVE_RETURN_ZEROS = True\n",
    "\n",
    "\n",
    "def parse_index_from_name(fname: str) -> str | None:\n",
    "    m = re.search(r\"_(\\d+)\\.npy$\", fname)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def normalize_wave(arr: np.ndarray):\n",
    "    \"\"\"\n",
    "    把 (3,3000) 或 (3000,3) 统一转为 (3,3000) float32\n",
    "    \"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    arr = np.squeeze(arr)\n",
    "\n",
    "    if arr.shape == (3, 3000):\n",
    "        pass\n",
    "    elif arr.shape == (3000, 3):\n",
    "        arr = arr.T\n",
    "    else:\n",
    "        # 允许一些可 squeeze 的情况\n",
    "        if arr.ndim == 3:\n",
    "            arr2 = np.squeeze(arr)\n",
    "            if arr2.shape == (3, 3000):\n",
    "                arr = arr2\n",
    "            elif arr2.shape == (3000, 3):\n",
    "                arr = arr2.T\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    return arr.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def _fix_time(feat_2d: np.ndarray, target_t: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    feat_2d: (F, T) -> (F, target_t)\n",
    "    \"\"\"\n",
    "    f, t = feat_2d.shape\n",
    "    if t == target_t:\n",
    "        return feat_2d\n",
    "    if t > target_t:\n",
    "        return feat_2d[:, :target_t]\n",
    "    # pad: 复制最后一帧\n",
    "    pad = np.repeat(feat_2d[:, -1:], repeats=(target_t - t), axis=1)\n",
    "    return np.concatenate([feat_2d, pad], axis=1)\n",
    "\n",
    "\n",
    "def compute_mfcc_1ch(x_1d: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x_1d: (3000,) -> (40,72)\n",
    "    \"\"\"\n",
    "    x_1d = np.asarray(x_1d, dtype=np.float32).reshape(-1)\n",
    "\n",
    "    if ZERO_WAVE_RETURN_ZEROS and np.all(x_1d == 0):\n",
    "        return np.zeros((TARGET_FEAT, TARGET_T), dtype=np.float32)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=x_1d,\n",
    "        sr=SR,\n",
    "        n_mfcc=N_MFCC,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP,\n",
    "        win_length=WIN_LENGTH,\n",
    "    )\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "    rms = librosa.feature.rms(y=x_1d, frame_length=N_FFT, hop_length=HOP)\n",
    "    log_energy = librosa.amplitude_to_db(rms)\n",
    "\n",
    "    feat = np.concatenate([mfcc, delta, delta2, log_energy], axis=0)  # (40,T)\n",
    "    feat = _fix_time(feat, TARGET_T).astype(np.float32, copy=False)\n",
    "    return feat\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not IN_ROOT.exists():\n",
    "        raise FileNotFoundError(f\"IN_ROOT not exists: {IN_ROOT}\")\n",
    "\n",
    "    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    subfolders = sorted([p for p in IN_ROOT.iterdir() if p.is_dir()])\n",
    "    print(f\"[INFO] Input root : {IN_ROOT}\")\n",
    "    print(f\"[INFO] Output root: {OUT_ROOT}\")\n",
    "    print(f\"[INFO] Found {len(subfolders)} subfolders\")\n",
    "\n",
    "    # ✅ 外层：文件夹进度（position=0）\n",
    "    folder_bar = tqdm(subfolders, desc=\"Folders\", unit=\"folder\", position=0, leave=True, dynamic_ncols=True)\n",
    "\n",
    "    for folder in folder_bar:\n",
    "        out_folder = OUT_ROOT / f\"{folder.name}_MFCC\"\n",
    "        out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        npy_files = sorted(folder.glob(\"*.npy\"))\n",
    "        if not npy_files:\n",
    "            tqdm.write(f\"[SKIP] {folder.name}: no .npy files\")\n",
    "            continue\n",
    "\n",
    "        kept, skipped = 0, 0\n",
    "\n",
    "        # ✅ 内层：当前文件夹内文件进度（position=1）\n",
    "        file_bar = tqdm(\n",
    "            npy_files,\n",
    "            desc=f\"{folder.name} (files)\",\n",
    "            unit=\"file\",\n",
    "            position=1,\n",
    "            leave=True,           # ✅ 关键：notebook里不容易被抹掉\n",
    "            dynamic_ncols=True,\n",
    "            mininterval=0.2\n",
    "        )\n",
    "\n",
    "        for i, fp in enumerate(file_bar, 1):\n",
    "            idx_str = parse_index_from_name(fp.name)\n",
    "            if idx_str is None:\n",
    "                idx_str = f\"{i:05d}\"\n",
    "\n",
    "            out_name = f\"{folder.name}_MFCC_{idx_str}.npy\"\n",
    "            out_path = out_folder / out_name\n",
    "\n",
    "            if SKIP_IF_EXISTS and out_path.exists():\n",
    "                skipped += 1\n",
    "                # ✅ 少量更新 postfix，不频繁刷新避免闪\n",
    "                if (i % 200) == 0:\n",
    "                    file_bar.set_postfix(kept=kept, skipped=skipped)\n",
    "                continue\n",
    "\n",
    "            # load\n",
    "            try:\n",
    "                arr = np.load(fp, allow_pickle=False)\n",
    "            except Exception:\n",
    "                skipped += 1\n",
    "                if (i % 200) == 0:\n",
    "                    file_bar.set_postfix(kept=kept, skipped=skipped)\n",
    "                continue\n",
    "\n",
    "            wave = normalize_wave(arr)\n",
    "            if wave is None:\n",
    "                skipped += 1\n",
    "                if (i % 200) == 0:\n",
    "                    file_bar.set_postfix(kept=kept, skipped=skipped)\n",
    "                continue\n",
    "\n",
    "            # mfcc (3,3000) -> (3,40,72)\n",
    "            try:\n",
    "                mfcc3 = np.stack([compute_mfcc_1ch(wave[ch]) for ch in range(3)], axis=0).astype(np.float32)\n",
    "            except Exception:\n",
    "                skipped += 1\n",
    "                if (i % 200) == 0:\n",
    "                    file_bar.set_postfix(kept=kept, skipped=skipped)\n",
    "                continue\n",
    "\n",
    "            # save\n",
    "            try:\n",
    "                np.save(out_path, mfcc3)\n",
    "                kept += 1\n",
    "            except Exception:\n",
    "                skipped += 1\n",
    "\n",
    "            if (i % 200) == 0:\n",
    "                file_bar.set_postfix(kept=kept, skipped=skipped)\n",
    "\n",
    "        # 结束当前文件夹\n",
    "        file_bar.set_postfix(kept=kept, skipped=skipped)\n",
    "        file_bar.close()\n",
    "\n",
    "        # 外层也显示一下当前文件夹统计\n",
    "        folder_bar.set_postfix_str(f\"{folder.name} kept={kept} skipped={skipped}\")\n",
    "        tqdm.write(f\"[OK] {folder.name} -> {out_folder.name} | kept={kept}, skipped={skipped}, total={len(npy_files)}\")\n",
    "\n",
    "    folder_bar.close()\n",
    "    print(\"[DONE] All folders processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "MFCC_ROOT = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement_MFCC\")\n",
    "\n",
    "# 随机种子（想复现实验就固定，不想就设 None）\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "def find_one_npy(folder: Path):\n",
    "    \"\"\"在该文件夹里随机找一个 .npy\"\"\"\n",
    "    files = sorted(folder.glob(\"*.npy\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    return random.choice(files)\n",
    "\n",
    "\n",
    "def plot_mfcc(mfcc: np.ndarray, title: str):\n",
    "    \"\"\"\n",
    "    mfcc 支持:\n",
    "      - (40, 72)\n",
    "      - (3, 40, 72)  (三分量各一张)\n",
    "      - (72, 40) / (72, 40, 3) 等少数情况会尽量纠正\n",
    "    \"\"\"\n",
    "    mfcc = np.asarray(mfcc)\n",
    "\n",
    "    if mfcc.ndim == 2:\n",
    "        # (40,72) 或 (72,40)\n",
    "        if mfcc.shape[0] == 40 and mfcc.shape[1] == 72:\n",
    "            to_show = mfcc\n",
    "        elif mfcc.shape[0] == 72 and mfcc.shape[1] == 40:\n",
    "            to_show = mfcc.T\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected 2D shape: {mfcc.shape}\")\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.imshow(to_show, origin=\"lower\", aspect=\"auto\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time frames\")\n",
    "        plt.ylabel(\"Feature bins\")\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    if mfcc.ndim == 3:\n",
    "        # (3,40,72) 或 (40,72,3) 等\n",
    "        if mfcc.shape[0] == 3 and mfcc.shape[1] == 40 and mfcc.shape[2] == 72:\n",
    "            m0, m1, m2 = mfcc[0], mfcc[1], mfcc[2]\n",
    "        elif mfcc.shape[-1] == 3 and mfcc.shape[0] == 40 and mfcc.shape[1] == 72:\n",
    "            m0, m1, m2 = mfcc[:, :, 0], mfcc[:, :, 1], mfcc[:, :, 2]\n",
    "        elif mfcc.shape[0] == 72 and mfcc.shape[1] == 40 and mfcc.shape[2] == 3:\n",
    "            # (72,40,3)\n",
    "            m0, m1, m2 = mfcc[:, :, 0].T, mfcc[:, :, 1].T, mfcc[:, :, 2].T\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected 3D shape: {mfcc.shape}\")\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        for ax, m, name in zip(axes, [m0, m1, m2], [\"ch0\", \"ch1\", \"ch2\"]):\n",
    "            im = ax.imshow(m, origin=\"lower\", aspect=\"auto\")\n",
    "            ax.set_title(f\"{title} | {name}\")\n",
    "            ax.set_xlabel(\"Time frames\")\n",
    "            ax.set_ylabel(\"Feature bins\")\n",
    "            fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Unsupported mfcc ndim={mfcc.ndim}, shape={mfcc.shape}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not MFCC_ROOT.exists():\n",
    "        raise FileNotFoundError(f\"MFCC_ROOT not found: {MFCC_ROOT}\")\n",
    "\n",
    "    subfolders = sorted([p for p in MFCC_ROOT.iterdir() if p.is_dir()])\n",
    "    print(f\"[INFO] MFCC root: {MFCC_ROOT}\")\n",
    "    print(f\"[INFO] Found {len(subfolders)} subfolders\\n\")\n",
    "\n",
    "    for folder in subfolders:\n",
    "        npy_path = find_one_npy(folder)\n",
    "        if npy_path is None:\n",
    "            print(f\"[SKIP] {folder.name}: no .npy files\")\n",
    "            continue\n",
    "\n",
    "        mfcc = np.load(npy_path, allow_pickle=False)\n",
    "        print(f\"=== Folder: {folder.name} ===\")\n",
    "        print(f\"File: {npy_path}\")\n",
    "        print(f\"Shape: {mfcc.shape}, dtype: {mfcc.dtype}\")\n",
    "        print(f\"min/max: {mfcc.min():.4f} / {mfcc.max():.4f}\\n\")\n",
    "\n",
    "        plot_mfcc(mfcc, title=f\"{folder.name}\\n{npy_path.name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# =========================\n",
    "# 根目录（输入=输出都在这里）\n",
    "# =========================\n",
    "ROOT = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement_MFCC\")\n",
    "\n",
    "# =========================\n",
    "# 固定标签顺序（你要求的）\n",
    "# =========================\n",
    "LABEL2ID_FIXED = {\n",
    "    \"Earthquake\": 0,\n",
    "    \"Explosion\": 1,\n",
    "    \"Collapse\": 2,\n",
    "    \"Noise\": 3,\n",
    "}\n",
    "\n",
    "LABEL_MAP_PATH = ROOT / \"label2id.json\"\n",
    "\n",
    "# 是否跳过已存在的 pt\n",
    "SKIP_PT_IF_EXISTS = False\n",
    "\n",
    "# 统一 MFCC 输出 shape\n",
    "TARGET_H, TARGET_W = 40, 72\n",
    "TARGET_C = 3\n",
    "\n",
    "# folder 名末尾是否带 \"_MFCC\"\n",
    "SUFFIX = \"_MFCC\"\n",
    "\n",
    "\n",
    "def label_from_folder(folder_name: str) -> str:\n",
    "    \"\"\"\n",
    "    从子文件夹名解析主类标签：\n",
    "      Earthquake_train_dataset_enhancement_MFCC -> Earthquake\n",
    "      Noise_Expert_B_test_dataset_MFCC          -> Noise\n",
    "    \"\"\"\n",
    "    base = folder_name\n",
    "    if base.endswith(SUFFIX):\n",
    "        base = base[: -len(SUFFIX)]\n",
    "    return base.split(\"_\")[0]\n",
    "\n",
    "\n",
    "def normalize_mfcc(arr: np.ndarray) -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    将输入 MFCC 统一成 (3,40,72) float32\n",
    "    支持：\n",
    "      - (3,40,72)\n",
    "      - (1,40,72) -> 复制成 (3,40,72)\n",
    "      - (40,72)   -> 扩成 (3,40,72)\n",
    "      - (72,40)   -> 转置再扩\n",
    "      - (40,72,3) / (72,40,3)\n",
    "    \"\"\"\n",
    "    a = np.asarray(arr)\n",
    "    a = np.squeeze(a)\n",
    "\n",
    "    # (3,40,72) 或 (1,40,72)\n",
    "    if a.ndim == 3 and a.shape[1:] == (TARGET_H, TARGET_W) and a.shape[0] in (1, 3):\n",
    "        if a.shape[0] == 1:\n",
    "            a = np.repeat(a, repeats=TARGET_C, axis=0)\n",
    "        return a.astype(np.float32, copy=False)\n",
    "\n",
    "    # (40,72) 或 (72,40)\n",
    "    if a.ndim == 2:\n",
    "        if a.shape == (TARGET_H, TARGET_W):\n",
    "            a = a[None, :, :]  # (1,40,72)\n",
    "            a = np.repeat(a, TARGET_C, axis=0)\n",
    "            return a.astype(np.float32, copy=False)\n",
    "        if a.shape == (TARGET_W, TARGET_H):\n",
    "            a = a.T\n",
    "            a = a[None, :, :]\n",
    "            a = np.repeat(a, TARGET_C, axis=0)\n",
    "            return a.astype(np.float32, copy=False)\n",
    "        return None\n",
    "\n",
    "    # (40,72,3)\n",
    "    if a.ndim == 3 and a.shape == (TARGET_H, TARGET_W, TARGET_C):\n",
    "        a = np.transpose(a, (2, 0, 1))\n",
    "        return a.astype(np.float32, copy=False)\n",
    "\n",
    "    # (72,40,3)\n",
    "    if a.ndim == 3 and a.shape == (TARGET_W, TARGET_H, TARGET_C):\n",
    "        a = np.transpose(a, (2, 1, 0))\n",
    "        return a.astype(np.float32, copy=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not ROOT.exists():\n",
    "        raise FileNotFoundError(f\"ROOT not found: {ROOT}\")\n",
    "\n",
    "    # 保存固定 label2id\n",
    "    with open(LABEL_MAP_PATH, \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(LABEL2ID_FIXED, fp, ensure_ascii=False, indent=2)\n",
    "    print(f\"[OK] Saved fixed label2id: {LABEL_MAP_PATH}\")\n",
    "    print(f\"[INFO] label2id = {LABEL2ID_FIXED}\")\n",
    "\n",
    "    subfolders = sorted([p for p in ROOT.iterdir() if p.is_dir()])\n",
    "    print(f\"[INFO] Found {len(subfolders)} subfolders in {ROOT}\")\n",
    "\n",
    "    # 逐子文件夹打包成 pt\n",
    "    for folder in tqdm(subfolders, desc=\"Packing subfolders\", unit=\"folder\"):\n",
    "        npy_files = sorted(folder.glob(\"*.npy\"))\n",
    "        if not npy_files:\n",
    "            tqdm.write(f\"[SKIP] {folder.name}: no .npy files\")\n",
    "            continue\n",
    "\n",
    "        label_name = label_from_folder(folder.name)\n",
    "        if label_name not in LABEL2ID_FIXED:\n",
    "            tqdm.write(f\"[SKIP] {folder.name}: unknown label '{label_name}', not in {list(LABEL2ID_FIXED.keys())}\")\n",
    "            continue\n",
    "\n",
    "        label_id = LABEL2ID_FIXED[label_name]\n",
    "\n",
    "        out_pt = ROOT / f\"{folder.name}.pt\"  # 保存到 ROOT\n",
    "        if SKIP_PT_IF_EXISTS and out_pt.exists():\n",
    "            tqdm.write(f\"[SKIP] pt exists: {out_pt}\")\n",
    "            continue\n",
    "\n",
    "        xs = []\n",
    "        files_kept = []\n",
    "        skipped = 0\n",
    "\n",
    "        file_bar = tqdm(npy_files, desc=f\"{folder.name}\", unit=\"file\", leave=False, dynamic_ncols=True)\n",
    "        for i, fp in enumerate(file_bar, 1):\n",
    "            try:\n",
    "                arr = np.load(fp, allow_pickle=False)\n",
    "            except Exception:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            mfcc = normalize_mfcc(arr)\n",
    "            if mfcc is None:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            xs.append(mfcc)\n",
    "            files_kept.append(fp.name)\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                file_bar.set_postfix(kept=len(xs), skipped=skipped)\n",
    "\n",
    "        file_bar.close()\n",
    "\n",
    "        if len(xs) == 0:\n",
    "            tqdm.write(f\"[FAIL] {folder.name}: loaded=0, skipped={skipped}\")\n",
    "            continue\n",
    "\n",
    "        x = torch.from_numpy(np.stack(xs, axis=0))  # (N,3,40,72)\n",
    "        y = torch.full((x.shape[0],), fill_value=label_id, dtype=torch.long)\n",
    "\n",
    "        payload = {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"label_name\": label_name,\n",
    "            \"label_id\": int(label_id),\n",
    "            \"files\": files_kept,\n",
    "            \"source_folder\": str(folder),\n",
    "        }\n",
    "\n",
    "        torch.save(payload, out_pt)\n",
    "        tqdm.write(\n",
    "            f\"[OK] Saved: {out_pt}\\n\"\n",
    "            f\"     label={label_name} (id={label_id}), kept={x.shape[0]}, skipped={skipped}, x.shape={tuple(x.shape)}, dtype={x.dtype}\"\n",
    "        )\n",
    "\n",
    "    print(\"[DONE] All .pt saved into ROOT\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "ROOT = Path(\"/home/zypei/DiTing2.0_dataset/remake_noise_experiment/EQ_EP_SS_Noise_datasets_enhancement_MFCC\")\n",
    "\n",
    "# ===== 你可随时调这些参数 =====\n",
    "seed = 0\n",
    "\n",
    "num_pt_files = 6          # 随机抽多少个 .pt 数据集文件\n",
    "samples_per_pt = 2        # 每个 .pt 里抽几条样本来打印\n",
    "\n",
    "# 打印具体数值的控制\n",
    "first_n_flat = 40         # flatten后前多少个数\n",
    "print_h = 8               # 打印小块：高方向前多少（例如 40）\n",
    "print_w = 12              # 打印小块：宽方向前多少（例如 72）\n",
    "\n",
    "# 如果你只想看某些名字包含的pt（比如只看Earthquake），设置关键字，否则 None\n",
    "only_name_contains = None  # 例如 \"Earthquake\" / \"Noise\" / \"Explosion\" / \"Collapse\"\n",
    "\n",
    "\n",
    "def tensor_stats(x: torch.Tensor):\n",
    "    x_cpu = x.detach().cpu()\n",
    "    s = {\n",
    "        \"shape\": tuple(x_cpu.shape),\n",
    "        \"dtype\": str(x_cpu.dtype),\n",
    "        \"device\": str(x.device),\n",
    "        \"min\": float(x_cpu.min().item()) if x_cpu.numel() else None,\n",
    "        \"max\": float(x_cpu.max().item()) if x_cpu.numel() else None,\n",
    "        \"mean\": float(x_cpu.mean().item()) if x_cpu.numel() else None,\n",
    "        \"std\": float(x_cpu.std().item()) if x_cpu.numel() else None,\n",
    "        \"any_nan\": bool(torch.isnan(x_cpu).any().item()) if x_cpu.is_floating_point() else False,\n",
    "        \"any_inf\": bool(torch.isinf(x_cpu).any().item()) if x_cpu.is_floating_point() else False,\n",
    "        \"all_zero_exact\": bool((x_cpu == 0).all().item()) if x_cpu.numel() else False,\n",
    "    }\n",
    "    return s\n",
    "\n",
    "\n",
    "def print_one_sample(x1: torch.Tensor, title: str = \"\"):\n",
    "    # x1: (C,H,W) 或 (C,T) 或其它\n",
    "    if title:\n",
    "        print(\"\\n\" + \"-\" * 90)\n",
    "        print(title)\n",
    "\n",
    "    st = tensor_stats(x1)\n",
    "    print(f\"Type: {type(x1)}\")\n",
    "    print(f\"Shape: {st['shape']}\")\n",
    "    print(f\"Dtype: {st['dtype']}\")\n",
    "    print(f\"Device: {st['device']}\")\n",
    "    print(f\"Min/Max: {st['min']} / {st['max']}\")\n",
    "    print(f\"Mean/Std: {st['mean']} / {st['std']}\")\n",
    "    print(f\"Any NaN? {st['any_nan']} | Any Inf? {st['any_inf']} | All-zero? {st['all_zero_exact']}\")\n",
    "\n",
    "    flat = x1.detach().cpu().flatten().numpy()\n",
    "    n = min(first_n_flat, flat.size)\n",
    "    print(f\"\\nFirst {n} values (flatten):\")\n",
    "    print(flat[:n])\n",
    "\n",
    "    # 打印一个小块（更直观）\n",
    "    x_cpu = x1.detach().cpu()\n",
    "    if x_cpu.ndim == 3:\n",
    "        # (C,H,W)\n",
    "        c0 = x_cpu[0]\n",
    "        h = min(print_h, c0.shape[0])\n",
    "        w = min(print_w, c0.shape[1])\n",
    "        print(f\"\\nBlock: channel0[:{h}, :{w}]\")\n",
    "        print(c0[:h, :w].numpy())\n",
    "    elif x_cpu.ndim == 2:\n",
    "        # (C,T) 或 (H,W)\n",
    "        h = min(print_h, x_cpu.shape[0])\n",
    "        w = min(print_w, x_cpu.shape[1])\n",
    "        print(f\"\\nBlock: [: {h}, : {w}]\")\n",
    "        print(x_cpu[:h, :w].numpy())\n",
    "\n",
    "\n",
    "def load_pt_any(pt_path: Path):\n",
    "    \"\"\"\n",
    "    支持两种格式：\n",
    "      1) dict: {'x': Tensor, 'y': Tensor, ...}\n",
    "      2) Tensor: 直接保存 Tensor\n",
    "    返回：x, y, meta_dict\n",
    "    \"\"\"\n",
    "    obj = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        x = obj.get(\"x\", None)\n",
    "        y = obj.get(\"y\", None)\n",
    "        meta = {k: v for k, v in obj.items() if k not in (\"x\", \"y\")}\n",
    "        if x is None:\n",
    "            raise ValueError(f\"{pt_path} is dict but missing key 'x'\")\n",
    "        return x, y, meta\n",
    "\n",
    "    if torch.is_tensor(obj):\n",
    "        x = obj\n",
    "        return x, None, {}\n",
    "\n",
    "    raise TypeError(f\"Unsupported pt format in {pt_path}: {type(obj)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    if not ROOT.exists():\n",
    "        raise FileNotFoundError(f\"ROOT not found: {ROOT}\")\n",
    "\n",
    "    pt_files = sorted(ROOT.glob(\"*.pt\"))\n",
    "    if only_name_contains:\n",
    "        pt_files = [p for p in pt_files if only_name_contains in p.name]\n",
    "\n",
    "    print(f\"[INFO] ROOT = {ROOT}\")\n",
    "    print(f\"[INFO] Found {len(pt_files)} .pt files\")\n",
    "\n",
    "    if len(pt_files) == 0:\n",
    "        print(\"[WARN] No .pt files found.\")\n",
    "        return\n",
    "\n",
    "    if len(pt_files) > num_pt_files:\n",
    "        chosen_pts = random.sample(pt_files, num_pt_files)\n",
    "    else:\n",
    "        chosen_pts = pt_files\n",
    "\n",
    "    for pt in chosen_pts:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"[PT] {pt}\")\n",
    "\n",
    "        try:\n",
    "            x, y, meta = load_pt_any(pt)\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] load pt: {pt} | err={e}\")\n",
    "            continue\n",
    "\n",
    "        # 打印 meta\n",
    "        print(\"=== BASIC INFO ===\")\n",
    "        print(f\"x: type={type(x)}, shape={tuple(x.shape)}, dtype={x.dtype}\")\n",
    "        if y is not None:\n",
    "            if torch.is_tensor(y):\n",
    "                print(f\"y: type={type(y)}, shape={tuple(y.shape)}, dtype={y.dtype}\")\n",
    "            else:\n",
    "                print(f\"y: type={type(y)}\")\n",
    "        else:\n",
    "            print(\"y: None (not saved)\")\n",
    "\n",
    "        if meta:\n",
    "            # 只打印 meta 的键，避免太长\n",
    "            print(f\"meta keys: {list(meta.keys())}\")\n",
    "            # 常见字段打印一下\n",
    "            for k in (\"label_name\", \"label_id\", \"source_folder\"):\n",
    "                if k in meta:\n",
    "                    print(f\"{k}: {meta[k]}\")\n",
    "        else:\n",
    "            print(\"meta: {}\")\n",
    "\n",
    "        # 全局统计（抽样）\n",
    "        print(\"\\n=== GLOBAL STATS (x) ===\")\n",
    "        stx = tensor_stats(x)\n",
    "        print(stx)\n",
    "\n",
    "        # 抽样打印几条\n",
    "        n_total = x.shape[0]\n",
    "        k = min(samples_per_pt, n_total)\n",
    "        idxs = random.sample(range(n_total), k) if n_total >= k else list(range(n_total))\n",
    "\n",
    "        print(\"\\n=== SAMPLE PREVIEW ===\")\n",
    "        for i in idxs:\n",
    "            title = f\"[SAMPLE idx={i}]\"\n",
    "            if y is not None and torch.is_tensor(y) and y.numel() == n_total:\n",
    "                title += f\" | y={int(y[i].item())}\"\n",
    "            print_one_sample(x[i], title=title)\n",
    "\n",
    "    print(\"\\n[DONE]\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
